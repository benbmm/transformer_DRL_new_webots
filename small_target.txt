六足機器人地形適應控制系統 - 小目標
核心目標
使用Transformer + PPO強化學習訓練六足機器人在會變換傾斜角度的平台保持機身水平，機器人會原地站在平台上，由transformer產生修正量控制膝關節以平衡機身，其餘關節保持固定角度

技術架構
1. AI架構

Policy Network: Transformer
訓練算法: PPO (Proximal Policy Optimization)
模擬環境: Webots
感測器限制: 僅使用IMU（慣性測量單元）

2. Transformer設計規格
python# 網絡架構
sequence_length = n 	# 根據控制頻率設定，n*頻率=1秒，n目前訂為50
state_dim = 6           # 由歐拉角計算六隻腳方向的分量，e¹ = (pitch + roll)√(1/2) - 前右腳，需要同時響應俯仰和翻滾，e² = roll - 右中腳，主要響應翻滾運動，e³ = (-pitch + roll)√(1/2) - 後右腳，翻滾為正，俯仰為負，e⁴ = (-pitch - roll)√(1/2) - 後左腳，兩個運動都為負向響應，e⁵ = -roll - 左中腳，翻滾的反向響應，e⁶ = (pitch - roll)√(1/2) - 前左腳，俯仰為正，翻滾為負
action_dim = 6         # 6維修正量 (6腿 × 1關節)，只有膝關節
reward_dim = 1          # 標量獎勵值
hidden_size = 128       # 隱藏層維度
n_layer = 3             # Transformer層數
n_head = 2              # 單頭注意力
dropout = 0.1           # Dropout率

# 輸入序列結構
input_sequence = [(state_t-n-1, action_t-n-1, reward_t-n-1), ..., (state_t, action_t, reward_t)]
3. 混合控制系統

控制架構: 固定角度訊號 + 強化學習
基礎步態: 原地站立，訊號固定0度，並且需要調整訊號方向、應用機身高度偏移
適應修正: Transformer學習地形適應修正量
最終輸出: at = ac + ar (固定角度訊號 + RL修正)

4. 獎勵函數設計
    (1) 穩定性獎勵(r_s) #pitch or roll>=0.785時加入懲罰函數
        公式： exp(-((abs(pitch)+abs(roll))/2)^2/0.1^2)

        目的： 鼓勵機器人在移動中保持機身水平、穩定
        條件： 無特殊條件
        說明： 利用IMU獲得pitch，roll
        權重： w_s

    (2) 跌倒懲罰 (p_s)
        公式： p_s = -1

        目的： 懲罰機器人跌倒
        條件： 機器人跌倒時，當pitch or roll>=0.524，或是移動到平台邊緣，利用機器人座標判斷，超出以下範圍要懲罰:-0.2<x<0.2 & -0.18<y<0.18
        說明： 懲罰項，確保機器人保持穩定

    (3) 總獎勵函數
        R=r_s+p_s

# 早期終止條件
- pitch or roll > 0.524 rad 
- 到達最大step=2000

5. 訓練過程數據保存
使用PyTorch中包含的或是其他現有的紀錄方式
包含內容: 優化器狀態 + 訓練統計 + 課程學習進度+獎勵組成
6.當需要重新採集數據時，需要使用robot.simulationReset()來重新載入環境，這樣不會讓controller執行中斷。
    wb_supervisor_simulation_reset
    CC++PythonJavaMATLAB
    from controller import Supervisor

    class Supervisor (Robot):
        def simulationReset(self):
        # ...
    Description
    reset the simulation

    The wb_supervisor_simulation_reset function sends a request to the simulator process, asking it to reset the simulation at the end of the step. The reset process is explained in detail in the User Guide, the only difference is that the supervisor and robot controllers are not restarted, if needed, they have to be restarted with the wb_supervisor_node_restart_controller function. You may wish to save some data in a file from your supervisor and robot controller programs in order to reload it when they restart.
7.再控制器中需要用Supervisor方式控制訓練平台，實現正弦波運動，訊號為0.2 * math.sin(1.0 * math.pi * current_time)

當前狀態

✅ 已完成: CPG六足控制代碼
✅ 已完成: 完整Webots機器人模型
✅ 已完成: 加入transformer


實作優先順序

設計Transformer架構 - 實現狀態-動作-獎勵序列處理
實作混合控制系統 - 整合CPG與Transformer
設計獎勵函數 
訓練實作
模型優化測試