懲罰項與終止條件還需要調整，終止條件比懲罰項嚴格，懲罰項要包含變數，機身高度低於某個閾值，控制角度大小

六足機器人地形適應控制系統 - 最終目標
核心目標
使用Transformer + PPO強化學習訓練六足機器人在未知地形保持機身水平，同時：

保持前進路線不受影響
前進速度越快越好


技術架構
1. AI架構

Policy Network: Transformer
訓練算法: PPO (Proximal Policy Optimization)
模擬環境: Webots
感測器限制: 僅使用IMU（慣性測量單元）

2. Transformer設計規格
python# 網絡架構
sequence_length = n 	# 根據控制頻率設定，n*頻率=1秒
state_dim = 6           # 由歐拉角計算六隻腳方向的分量，e¹ = (pitch + roll)√(1/2) - 前右腳，需要同時響應俯仰和翻滾，e² = roll - 右中腳，主要響應翻滾運動，e³ = (-pitch + roll)√(1/2) - 後右腳，翻滾為正，俯仰為負，e⁴ = (-pitch - roll)√(1/2) - 後左腳，兩個運動都為負向響應，e⁵ = -roll - 左中腳，翻滾的反向響應，e⁶ = (pitch - roll)√(1/2) - 前左腳，俯仰為正，翻滾為負
action_dim = 12         # 12維修正量 (6腿 × 2關節)，膝關節與踝關節
reward_dim = 1          # 標量獎勵值
hidden_size = 128       # 隱藏層維度
n_layer = 3             # Transformer層數
n_head = 2              # 單頭注意力
dropout = 0.1           # Dropout率

# 輸入序列結構
input_sequence = [(state_t-n-1, action_t-n-1, reward_t-n-1), ..., (state_t, action_t, reward_t)]
3. 混合控制系統

控制架構: CPG + 強化學習
基礎步態: CPG生成穩定基礎運動模式（已調優）
適應修正: Transformer學習地形適應修正量
最終輸出: at = ac + ar (CPG輸出 + RL修正)
修正量限制: ±0.6

4. 獎勵函數設計
    (1) 方向獎勵 (r_θ)  #abs(θ)>0.524時加入懲罰函數
        公式： r_θ = max(-abs(θ)/0.524+1,0)

        目的： 鼓勵機器人朝正確方向移動
        條件： 無特殊條件
        說明： 當機器人朝向與目標方向的夾角小於30°時給予正獎勵
        權重：w_θ
        方法： 使用IMU的yaw數據判斷目前前進方向與初始的前進方向夾角(θ)

    (2) 速度獎勵 (r_v)  #v<0.01時加入懲罰函數
        公式： r_v = min(v,1)

        目的： 鼓勵機器人以較高速度前進
        條件： 無特殊條件
        說明： 當 v = 0 時 r_v = 0，當 v -> 1時 r_v -> 1
        權重：w_v
        方法： 位置變化率計算 v 

    (3) 穩定性獎勵(r_s) #pitch or roll>0.785時加入懲罰函數
        公式： min(exp(-max(|pitch|,|roll|)/0.03),1)

        目的： 鼓勵機器人在移動中保持機身水平、穩定，最好pitch & roll小於0.03
        條件： 當pitch or roll <=0.05時
        說明： 用pitch與roll計算獎勵
        權重：w_s
        方法：利用IMU獲得pitch，roll

    (4) 高度獎勵(r_h) 
        公式： 

        目的： 鼓勵機器人在移動中抬高機身高度
        條件： 
        說明： 
        權重：w_h
        方法：

    (4) 方向懲罰 (p_θ)
        公式： p_θ = -10

        目的： 懲罰前進方向偏離過大
        條件： 機器人偏離過大，也就是yaw>=0.524
        說明： 懲罰項，確保機器人保持前進方向
        權重：w_θ

    (5) 速度懲罰 (p_v)
        公式： p_v = -10

        目的： 懲罰機器人倒退
        條件： 機器人倒退，也就是v<=0.01
        說明： 懲罰項，確保機器人前進
        權重：w_v

    (6) 跌倒懲罰 (p_s)
        公式： p_s = -10

        目的： 懲罰機器人跌倒
        條件： 機器人跌倒時，也就是pitch or roll>=0.785
        說明： 懲罰項，確保機器人保持穩定
        權重：w_s

權重分配
穩定性獎勵w_s: 0.4    # 最高優先級
方向維持獎勵w_θ: 0.3   # 次要優先級  
速度獎勵w_v: 0.3      # 最低優先級

# 早期終止條件
- 機身傾斜 > 0.52 rad (≈30°)
- 方向偏差 > 0.35 rad (≈20°)  
- 前進速度 < 0.1 m/s 持續1秒
5. 漸進式訓練策略
python# 階段1: 平地訓練
成功標準: 實做後看情況再決定，先設定固定的代
目標是能夠正常前進並且速度有提升後就進行下一步

# 階段3: 不平地形
成功標準: 實做後看情況再決定

# 階段4: 隨機複雜地形  
成功標準: 實做後看情況再決定

# 參數轉移策略
- 保持所有層可訓練（不凍結任何參數）
- 新階段學習率衰減至0.3倍
- 暖身訓練100個episode
6. 訓練過程數據保存
使用PyTorch中包含的或是其他現有的紀錄方式
包含內容: 優化器狀態 + 訓練統計 + 課程學習進度+獎勵組成

當前狀態

✅ 已完成: CPG六足控制代碼
✅ 已完成: 完整Webots機器人模型


實作優先順序

設計Transformer架構 - 實現狀態-動作-獎勵序列處理
實作混合控制系統 - 整合CPG與Transformer
設計獎勵函數 - 實現嚴苛指標的平衡設計
漸進式訓練實作 - 建立多階段地形課程
模型優化測試 - 達到最終嚴苛性能指標