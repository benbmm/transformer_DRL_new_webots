"""
Webots æ§åˆ¶å™¨ç¨‹å¼ - å…­è¶³æ©Ÿå™¨äºº TrXL-PPO è¨“ç·´
æª”æ¡ˆä½ç½®: controllers/rl_training_controller/rl_training_controller.py
"""

import sys
import os
import time  
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# å°å…¥ç’°å¢ƒå’Œ TrXL-PPO ç›¸é—œæ¨¡çµ„
from hexapod_env import HexapodBalanceEnv
from ppo_trxl import Agent, Args
from dataclasses import dataclass
from collections import deque
import tyro
from torch.utils.tensorboard import SummaryWriter

@dataclass 
class WebotsTrXLArgs(Args):
    """Webots å°ˆç”¨çš„ TrXL-PPO åƒæ•¸"""
    # è¦†è“‹åŸåƒæ•¸ä»¥é©æ‡‰å–®ç’°å¢ƒ
    num_envs: int = 1                    # å–®ç’°å¢ƒ
    num_steps: int = 2048               # å¢åŠ æ­¥æ•¸è£œå„Ÿä¸¦è¡Œåº¦
    batch_size: int = 2048              # 1 * 2048  
    minibatch_size: int = 256           # èª¿æ•´å°æ‰¹æ¬¡å¤§å°
    num_minibatches: int = 8            # 2048 / 256 = 8
    
    # èª¿æ•´è¨“ç·´åƒæ•¸
    total_timesteps: int = 10000000     # é™ä½ç¸½è¨“ç·´æ­¥æ•¸
    init_lr: float = 3e-4               # ç•¥å¾®èª¿é«˜å­¸ç¿’ç‡
    final_lr: float = 1e-5
    update_epochs: int = 4              # å¢åŠ æ›´æ–°è¼ªæ•¸
    
    # Transformer åƒæ•¸
    trxl_memory_length: int = 64        # é©åˆæ©Ÿå™¨äººä»»å‹™çš„è¨˜æ†¶é•·åº¦
    trxl_dim: int = 256                 # é™ä½ç¶­åº¦ç¯€çœè¨ˆç®—
    trxl_num_layers: int = 2            # æ¸›å°‘å±¤æ•¸
    
    # Webots å°ˆç”¨åƒæ•¸
    max_episode_steps: int = 2000       # episode æœ€å¤§æ­¥æ•¸
    save_interval: int = 100            # æ¨¡å‹ä¿å­˜é–“éš”(iterations)
    eval_interval: int = 50             # è©•ä¼°é–“éš”(iterations)
    webots_mode: str = "training"       # "training" æˆ– "evaluation"


class SingleEnvMemoryManager:
    """å–®ç’°å¢ƒè¨˜æ†¶é«”ç®¡ç†å™¨ - ç°¡åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, max_episode_steps, memory_length, num_layers, embed_dim):
        self.max_episode_steps = max_episode_steps
        self.memory_length = memory_length
        self.num_layers = num_layers
        self.embed_dim = embed_dim
        
        # ç•¶å‰episodeçš„è¨˜æ†¶é«”
        self.current_episode_memory = torch.zeros(
            max_episode_steps, num_layers, embed_dim
        )
        self.current_episode_step = 0
        
        # ç”Ÿæˆè¨˜æ†¶é«”é®ç½©å’Œç´¢å¼•ï¼ˆä¸€æ¬¡æ€§è¨ˆç®—ï¼‰
        self._setup_memory_patterns()
        
    def _setup_memory_patterns(self):
        """è¨­ç½®è¨˜æ†¶é«”é®ç½©å’Œç´¢å¼•æ¨¡å¼"""
        # ç”Ÿæˆä¸‹ä¸‰è§’é®ç½©
        self.memory_masks = []
        for i in range(self.memory_length):
            mask = torch.zeros(self.memory_length, dtype=torch.bool)
            mask[:i] = True  # åªèƒ½çœ‹åˆ°ä¹‹å‰çš„æ­¥é©Ÿ
            self.memory_masks.append(mask)
        
        # ç”Ÿæˆæ»‘å‹•çª—å£ç´¢å¼•
        self.memory_indices = []
        for step in range(self.max_episode_steps):
            # è¨ˆç®—ç•¶å‰æ­¥é©Ÿçš„è¨˜æ†¶é«”çª—å£
            start_idx = max(0, step - self.memory_length + 1)
            end_idx = step + 1
            
            # å‰µå»ºç´¢å¼•æ•¸çµ„
            indices = torch.arange(start_idx, end_idx)
            
            # å¦‚æœä¸è¶³memory_lengthï¼Œç”¨0å¡«å……
            if len(indices) < self.memory_length:
                padding = torch.zeros(self.memory_length - len(indices), dtype=torch.long)
                indices = torch.cat([padding, indices])
            
            self.memory_indices.append(indices)
        
        self.memory_indices = torch.stack(self.memory_indices)
    
    def get_memory_window(self, step):
        """ç²å–æŒ‡å®šæ­¥é©Ÿçš„è¨˜æ†¶é«”çª—å£"""
        step = min(step, self.max_episode_steps - 1)
        indices = self.memory_indices[step]
        mask = self.memory_masks[min(step, self.memory_length - 1)]
        
        # æå–è¨˜æ†¶é«”çª—å£
        memory_window = self.current_episode_memory[indices]  # [memory_length, num_layers, embed_dim]
        
        return memory_window.unsqueeze(0), mask.unsqueeze(0), indices.unsqueeze(0)
    
    def update_memory(self, step, new_memory):
        """æ›´æ–°æŒ‡å®šæ­¥é©Ÿçš„è¨˜æ†¶é«”"""
        if step < self.max_episode_steps:
            self.current_episode_memory[step] = new_memory.squeeze(0)
            self.current_episode_step = step
    
    def reset(self):
        """é‡ç½®episodeè¨˜æ†¶é«”"""
        self.current_episode_memory.zero_()
        self.current_episode_step = 0


class SingleEnvRolloutBuffer:
    """å–®ç’°å¢ƒrolloutç·©è¡å€"""
    
    def __init__(self, num_steps, obs_shape, action_dim, memory_length, num_layers, embed_dim):
        self.num_steps = num_steps
        self.action_dim = action_dim
        
        # åŸºæœ¬æ•¸æ“šç·©è¡å€
        self.observations = torch.zeros((num_steps,) + obs_shape)
        self.actions = torch.zeros((num_steps, action_dim))
        self.log_probs = torch.zeros((num_steps, action_dim))
        self.rewards = torch.zeros(num_steps)
        self.values = torch.zeros(num_steps)
        self.dones = torch.zeros(num_steps, dtype=torch.bool)
        
        # è¨˜æ†¶é«”ç›¸é—œç·©è¡å€
        self.memory_windows = torch.zeros((num_steps, memory_length, num_layers, embed_dim))
        self.memory_masks = torch.zeros((num_steps, memory_length), dtype=torch.bool)
        self.memory_indices = torch.zeros((num_steps, memory_length), dtype=torch.long)
        
        # Episodeé‚Šç•Œè¿½è¹¤
        self.episode_starts = torch.zeros(num_steps, dtype=torch.bool)
        self.episode_ids = torch.zeros(num_steps, dtype=torch.long)
        
        self.step = 0
        self.current_episode_id = 0
    
    def add(self, obs, action, log_prob, reward, value, done, 
            memory_window, memory_mask, memory_indices, episode_start=False):
        """æ·»åŠ ä¸€æ­¥æ•¸æ“š"""
        if self.step >= self.num_steps:
            raise IndexError("Buffer is full")
        
        self.observations[self.step] = obs
        self.actions[self.step] = action
        self.log_probs[self.step] = log_prob
        self.rewards[self.step] = reward
        self.values[self.step] = value
        self.dones[self.step] = done
        
        self.memory_windows[self.step] = memory_window.squeeze(0)
        self.memory_masks[self.step] = memory_mask.squeeze(0)
        self.memory_indices[self.step] = memory_indices.squeeze(0)
        
        self.episode_starts[self.step] = episode_start
        self.episode_ids[self.step] = self.current_episode_id
        
        if done:
            self.current_episode_id += 1
        
        self.step += 1
    
    def get_batch_data(self):
        """ç²å–æ‰¹æ¬¡æ•¸æ“šç”¨æ–¼è¨“ç·´"""
        return {
            'observations': self.observations[:self.step],
            'actions': self.actions[:self.step],
            'log_probs': self.log_probs[:self.step],
            'rewards': self.rewards[:self.step],
            'values': self.values[:self.step],
            'dones': self.dones[:self.step],
            'memory_windows': self.memory_windows[:self.step],
            'memory_masks': self.memory_masks[:self.step],
            'memory_indices': self.memory_indices[:self.step],
            'episode_starts': self.episode_starts[:self.step],
            'episode_ids': self.episode_ids[:self.step],
        }
    
    def reset(self):
        """é‡ç½®ç·©è¡å€"""
        self.step = 0
        self.current_episode_id = 0


class WebotsTrainer:
    """Webots ç’°å¢ƒä¸‹çš„ TrXL-PPO è¨“ç·´å™¨ - ä¿®æ­£ç‰ˆæœ¬"""
    
    def __init__(self, args):
        self.args = args
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device("cuda" if args.cuda and torch.cuda.is_available() else "cpu")
        torch.set_default_device(self.device)
        
        # å‰µå»ºç’°å¢ƒ
        self.env = HexapodBalanceEnv(max_episode_steps=args.max_episode_steps)
        
        # æª¢æŸ¥ç’°å¢ƒå…¼å®¹æ€§
        self._validate_environment()
        
        # å‰µå»ºæ™ºèƒ½é«”
        if hasattr(self.env.action_space, 'shape'):
            action_space_shape = self.env.action_space.shape
        else:
            action_space_shape = (self.env.action_space.n,)

        self.agent = Agent(
            args, 
            self.env.observation_space, 
            action_space_shape,
            args.max_episode_steps
        ).to(self.device)
        
        # å‰µå»ºå„ªåŒ–å™¨
        self.optimizer = optim.AdamW(self.agent.parameters(), lr=args.init_lr)
        
        # ä¿®æ­£ï¼šå‰µå»ºè¨˜æ†¶é«”ç®¡ç†å™¨
        self.memory_manager = SingleEnvMemoryManager(
            args.max_episode_steps,
            args.trxl_memory_length,
            args.trxl_num_layers,
            args.trxl_dim
        )
        
        # ä¿®æ­£ï¼šå‰µå»ºrolloutç·©è¡å€
        self.rollout_buffer = SingleEnvRolloutBuffer(
            args.num_steps,
            self.env.observation_space.shape,
            action_space_shape[0],  # é€£çºŒå‹•ä½œç¶­åº¦
            args.trxl_memory_length,
            args.trxl_num_layers,
            args.trxl_dim
        )
        
        # è¨“ç·´ç‹€æ…‹
        self.global_step = 0
        self.iteration = 0
        self.episode_count = 0
        
        # ç›£æ§çµ±è¨ˆ
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.episode_infos = deque(maxlen=100)
        
        # TensorBoard
        run_name = f"webots_hexapod_trxl_ppo_{int(time.time())}"
        self.writer = SummaryWriter(f"runs/{run_name}")
        
        print("ğŸš€ Webots TrXL-PPO è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š ç’°å¢ƒ: {self.env.observation_space} -> {self.env.action_space}")
        print(f"ğŸ§  æ™ºèƒ½é«”: {args.trxl_dim}d Transformer, {args.trxl_num_layers} layers")
        print(f"ğŸ’¾ è¨­å‚™: {self.device}")
        print(f"ğŸ“ˆ TensorBoard: runs/{run_name}")

    def _validate_environment(self):
        """é©—è­‰ç’°å¢ƒå…¼å®¹æ€§"""
        # æª¢æŸ¥è§€æ¸¬ç©ºé–“
        assert len(self.env.observation_space.shape) == 1, "éœ€è¦å‘é‡è§€æ¸¬ç©ºé–“"
        
        # æª¢æŸ¥å‹•ä½œç©ºé–“  
        assert hasattr(self.env.action_space, 'shape'), "éœ€è¦é€£çºŒå‹•ä½œç©ºé–“"
        
        # æ¸¬è©¦ç’°å¢ƒ
        obs, info = self.env.reset()
        assert obs.shape == self.env.observation_space.shape, "è§€æ¸¬ç¶­åº¦ä¸åŒ¹é…"
        
        # æ¸¬è©¦å‹•ä½œ
        test_action = self.env.action_space.sample()
        obs, reward, terminated, truncated, info = self.env.step(test_action)
        
        print("âœ… ç’°å¢ƒå…¼å®¹æ€§é©—è­‰é€šé")

    def collect_rollout(self):
        """æ”¶é›†ä¸€å€‹ rollout çš„ç¶“é©— - ä¿®æ­£ç‰ˆæœ¬"""
        # é‡ç½®rolloutç·©è¡å€
        self.rollout_buffer.reset()
        
        # ç²å–ç•¶å‰è§€æ¸¬
        if not hasattr(self, 'current_obs'):
            obs, info = self.env.reset()
            self.current_obs = torch.tensor(obs, dtype=torch.float32)
            self.current_done = False
            self.memory_manager.reset()
            episode_start = True
        else:
            episode_start = False
        
        # æ”¶é›†ç¶“é©—
        for step in range(self.args.num_steps):
            self.global_step += 1
            
            # ç²å–è¨˜æ†¶é«”çª—å£
            memory_window, memory_mask, memory_indices = self.memory_manager.get_memory_window(
                self.memory_manager.current_episode_step
            )
            
            # æ™ºèƒ½é«”æ±ºç­–
            with torch.no_grad():
                obs_tensor = self.current_obs.unsqueeze(0)
                action, log_prob, _, value, new_memory = self.agent.get_action_and_value(
                    obs_tensor,
                    memory_window,
                    memory_mask,
                    memory_indices
                )
                
                # æ›´æ–°è¨˜æ†¶é«”
                self.memory_manager.update_memory(
                    self.memory_manager.current_episode_step, 
                    new_memory
                )
            
            # åŸ·è¡Œå‹•ä½œ
            obs, reward, terminated, truncated, info = self.env.step(action[0].cpu().numpy())
            done = terminated or truncated
            
            # å­˜å„²æ•¸æ“šåˆ°ç·©è¡å€
            self.rollout_buffer.add(
                obs=self.current_obs,
                action=action[0],
                log_prob=log_prob[0],
                reward=reward,
                value=value[0],
                done=done,
                memory_window=memory_window,
                memory_mask=memory_mask,
                memory_indices=memory_indices,
                episode_start=episode_start
            )
            
            # æ›´æ–°ç‹€æ…‹
            self.current_obs = torch.tensor(obs, dtype=torch.float32)
            self.current_done = done
            episode_start = False
            
            # Episode ç®¡ç†
            if done:
                # è¨˜éŒ„ episode çµæœ
                self.episode_count += 1
                episode_info = {
                    'episode_reward': sum([self.rollout_buffer.rewards[i].item() 
                                         for i in range(max(0, step - self.memory_manager.current_episode_step), step + 1)]),
                    'episode_length': self.memory_manager.current_episode_step + 1,
                    'reason': info.get('reason', ''),
                    'stability_reward': info.get('stability_reward', 0),
                    'penalty': info.get('penalty', 0)
                }
                self.episode_infos.append(episode_info)
                
                # é‡ç½®ç’°å¢ƒå’Œè¨˜æ†¶é«”
                obs, info = self.env.reset()
                self.current_obs = torch.tensor(obs, dtype=torch.float32)
                self.current_done = False
                self.memory_manager.reset()
                episode_start = True
            else:
                self.memory_manager.current_episode_step += 1
        
        return self.rollout_buffer.get_batch_data()

    def compute_advantages(self, rollout_data):
        """è¨ˆç®— GAE advantages - ä¿®æ­£ç‰ˆæœ¬"""
        rewards = rollout_data['rewards']
        values = rollout_data['values']
        dones = rollout_data['dones']
        
        # è¨ˆç®—ä¸‹ä¸€å€‹å€¼
        with torch.no_grad():
            memory_window, memory_mask, memory_indices = self.memory_manager.get_memory_window(
                self.memory_manager.current_episode_step
            )
            
            next_value = self.agent.get_value(
                self.current_obs.unsqueeze(0),
                memory_window,
                memory_mask,
                memory_indices
            )[0]
        
        # GAE è¨ˆç®—
        advantages = torch.zeros_like(rewards)
        lastgaelam = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                nextnonterminal = 1.0 - float(self.current_done)
                nextvalues = next_value
            else:
                nextnonterminal = 1.0 - dones[t + 1].float()
                nextvalues = values[t + 1]
            
            delta = rewards[t] + self.args.gamma * nextvalues * nextnonterminal - values[t]
            advantages[t] = lastgaelam = delta + self.args.gamma * self.args.gae_lambda * nextnonterminal * lastgaelam
        
        returns = advantages + values
        
        return advantages, returns

    def update_policy(self, rollout_data, advantages, returns):
        """æ›´æ–°ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬"""
        # æº–å‚™è¨“ç·´æ•¸æ“š
        b_obs = rollout_data['observations']
        b_actions = rollout_data['actions']
        b_log_probs = rollout_data['log_probs']
        b_values = rollout_data['values']
        b_memory_windows = rollout_data['memory_windows']
        b_memory_masks = rollout_data['memory_masks']
        b_memory_indices = rollout_data['memory_indices']
        
        batch_size = len(b_obs)
        
        # å­¸ç¿’ç‡é€€ç«
        frac = 1.0 - (self.global_step - self.args.num_steps) / self.args.total_timesteps
        frac = max(0.0, frac)
        lr = frac * self.args.init_lr + (1 - frac) * self.args.final_lr
        for param_group in self.optimizer.param_groups:
            param_group["lr"] = lr
        
        # ç†µç³»æ•¸é€€ç«
        ent_coef = frac * self.args.init_ent_coef + (1 - frac) * self.args.final_ent_coef
        
        clipfracs = []
        
        for epoch in range(self.args.update_epochs):
            # ä¿®æ­£ï¼šæ­£ç¢ºçš„æ‰¹æ¬¡è™•ç†
            indices = torch.randperm(batch_size)
            
            for start in range(0, batch_size, self.args.minibatch_size):
                end = min(start + self.args.minibatch_size, batch_size)
                mb_inds = indices[start:end]
                
                # æå–minibatchæ•¸æ“š
                mb_obs = b_obs[mb_inds]
                mb_actions = b_actions[mb_inds]
                mb_log_probs = b_log_probs[mb_inds]
                mb_values = b_values[mb_inds]
                mb_memory_windows = b_memory_windows[mb_inds]
                mb_memory_masks = b_memory_masks[mb_inds]
                mb_memory_indices = b_memory_indices[mb_inds]
                mb_advantages = advantages[mb_inds]
                mb_returns = returns[mb_inds]
                
                # å‰å‘å‚³æ’­
                _, new_log_probs, entropy, new_values, _ = self.agent.get_action_and_value(
                    mb_obs,
                    mb_memory_windows,
                    mb_memory_masks,
                    mb_memory_indices,
                    mb_actions
                )
                
                # ç­–ç•¥æå¤±
                if self.args.norm_adv:
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
                
                # ä¿®æ­£ï¼šè™•ç†é€£çºŒå‹•ä½œçš„log_probs
                logratio = new_log_probs - mb_log_probs
                ratio = torch.exp(logratio)
                
                # ç‚ºæ¯å€‹å‹•ä½œç¶­åº¦è¨ˆç®—å„ªå‹¢
                mb_advantages_expanded = mb_advantages.unsqueeze(1).expand_as(ratio)
                
                # PPO æˆªæ–·
                pg_loss1 = -mb_advantages_expanded * ratio
                pg_loss2 = -mb_advantages_expanded * torch.clamp(
                    ratio, 1 - self.args.clip_coef, 1 + self.args.clip_coef
                )
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
                
                # åƒ¹å€¼æå¤±
                v_loss = ((new_values - mb_returns) ** 2).mean()
                
                # ç†µæå¤±
                entropy_loss = entropy.mean()
                
                # ç¸½æå¤±
                loss = pg_loss - ent_coef * entropy_loss + self.args.vf_coef * v_loss
                
                # åå‘å‚³æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
                self.optimizer.step()
                
                # è¨˜éŒ„çµ±è¨ˆ
                with torch.no_grad():
                    clipfracs += [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]
        
        return {
            'policy_loss': pg_loss.item(),
            'value_loss': v_loss.item(), 
            'entropy_loss': entropy_loss.item(),
            'clipfrac': np.mean(clipfracs),
            'learning_rate': lr,
            'entropy_coef': ent_coef
        }

    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        start_time = time.time()
        
        num_iterations = self.args.total_timesteps // self.args.batch_size
        
        for iteration in range(1, num_iterations + 1):
            self.iteration = iteration
            
            # æ”¶é›†ç¶“é©—
            rollout_data = self.collect_rollout()
            
            # è¨ˆç®— advantages
            advantages, returns = self.compute_advantages(rollout_data)
            
            # æ›´æ–°ç­–ç•¥
            update_info = self.update_policy(rollout_data, advantages, returns)
            
            # è¨˜éŒ„çµ±è¨ˆ
            self._log_training_stats(update_info, start_time)
            
            # ä¿å­˜æª¢æŸ¥é»
            if iteration % self.args.save_interval == 0:
                self._save_checkpoint(iteration)
            
            # è©•ä¼°
            if iteration % self.args.eval_interval == 0:
                self._evaluate()
        
        print("âœ… è¨“ç·´å®Œæˆ!")
        self._save_checkpoint(iteration, final=True)

    def _log_training_stats(self, update_info, start_time):
        """è¨˜éŒ„è¨“ç·´çµ±è¨ˆ"""
        if len(self.episode_infos) > 0:
            # Episode çµ±è¨ˆ
            episode_rewards = [info['episode_reward'] for info in self.episode_infos]
            episode_lengths = [info['episode_length'] for info in self.episode_infos]
            
            avg_reward = np.mean(episode_rewards[-10:]) if episode_rewards else 0
            avg_length = np.mean(episode_lengths[-10:]) if episode_lengths else 0
            
            # æ‰“å°é€²åº¦
            sps = int(self.global_step / (time.time() - start_time))
            print(f"è¿­ä»£ {self.iteration:4d} | æ­¥æ•¸ {self.global_step:8d} | "
                  f"çå‹µ {avg_reward:6.2f} | é•·åº¦ {avg_length:4.0f} | "
                  f"SPS {sps:4d} | ç­–ç•¥æå¤± {update_info['policy_loss']:.3f}")
            
            # TensorBoard è¨˜éŒ„
            self.writer.add_scalar("episode/reward_mean", avg_reward, self.global_step)
            self.writer.add_scalar("episode/length_mean", avg_length, self.global_step)
            self.writer.add_scalar("episode/count", len(self.episode_infos), self.global_step)
            
        # è¨“ç·´çµ±è¨ˆ
        for key, value in update_info.items():
            self.writer.add_scalar(f"training/{key}", value, self.global_step)
        
        self.writer.add_scalar("performance/sps", sps, self.global_step)

    def _save_checkpoint(self, iteration, final=False):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'iteration': iteration,
            'global_step': self.global_step,
            'agent_state_dict': self.agent.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'args': self.args
        }
        
        filename = f"checkpoint_final.pt" if final else f"checkpoint_{iteration}.pt"
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {filename}")

    def _evaluate(self):
        """è©•ä¼°æ™ºèƒ½é«”æ€§èƒ½"""
        print("ğŸ” é€²è¡Œè©•ä¼°...")
        eval_episodes = 5
        eval_rewards = []
        eval_lengths = []
        
        for _ in range(eval_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            # é‡ç½®è©•ä¼°è¨˜æ†¶é«”
            eval_memory_manager = SingleEnvMemoryManager(
                self.args.max_episode_steps,
                self.args.trxl_memory_length,
                self.args.trxl_num_layers,
                self.args.trxl_dim
            )
            
            while not done and episode_length < self.args.max_episode_steps:
                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
                
                # ç²å–è¨˜æ†¶é«”çª—å£
                memory_window, memory_mask, memory_indices = eval_memory_manager.get_memory_window(
                    eval_memory_manager.current_episode_step
                )
                
                with torch.no_grad():
                    action, _, _, _, new_memory = self.agent.get_action_and_value(
                        obs_tensor, memory_window, memory_mask, memory_indices
                    )
                    eval_memory_manager.update_memory(
                        eval_memory_manager.current_episode_step, new_memory
                    )
                
                obs, reward, terminated, truncated, info = self.env.step(action[0].cpu().numpy())
                done = terminated or truncated
                episode_reward += reward
                episode_length += 1
                eval_memory_manager.current_episode_step += 1
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
        
        avg_eval_reward = np.mean(eval_rewards)
        avg_eval_length = np.mean(eval_lengths)
        
        print(f"ğŸ“Š è©•ä¼°çµæœ: å¹³å‡çå‹µ {avg_eval_reward:.2f}, å¹³å‡é•·åº¦ {avg_eval_length:.1f}")
        
        self.writer.add_scalar("evaluation/reward_mean", avg_eval_reward, self.global_step)
        self.writer.add_scalar("evaluation/length_mean", avg_eval_length, self.global_step)


def main():
    """ä¸»å‡½æ•¸"""
    # è§£æåƒæ•¸
    args = tyro.cli(WebotsTrXLArgs)
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)
    
    # å‰µå»ºä¸¦é‹è¡Œè¨“ç·´å™¨
    trainer = WebotsTrainer(args)
    
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\nâš ï¸  è¨“ç·´è¢«ä¸­æ–·")
        trainer._save_checkpoint(trainer.iteration, final=True)
    except Exception as e:
        print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
        raise
    finally:
        trainer.writer.close()
        trainer.env.close()


if __name__ == "__main__":
    main()