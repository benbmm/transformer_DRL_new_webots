"""
Webots æ§åˆ¶å™¨ç¨‹å¼ - å…­è¶³æ©Ÿå™¨äºº TrXL-PPO è¨“ç·´
æª”æ¡ˆä½ç½®: controllers/rl_training_controller/rl_training_controller.py
"""

import sys
import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# å°å…¥ç’°å¢ƒå’Œ TrXL-PPO ç›¸é—œæ¨¡çµ„
from hexapod_env import HexapodBalanceEnv
from ppo_trxl import Agent, Args
from dataclasses import dataclass
from collections import deque
import tyro
from torch.utils.tensorboard import SummaryWriter

@dataclass 
class WebotsTrXLArgs(Args):
    """Webots å°ˆç”¨çš„ TrXL-PPO åƒæ•¸"""
    # è¦†è“‹åŸåƒæ•¸ä»¥é©æ‡‰å–®ç’°å¢ƒ
    num_envs: int = 1                    # å–®ç’°å¢ƒ
    num_steps: int = 2048               # å¢åŠ æ­¥æ•¸è£œå„Ÿä¸¦è¡Œåº¦
    batch_size: int = 2048              # 1 * 2048  
    minibatch_size: int = 256           # èª¿æ•´å°æ‰¹æ¬¡å¤§å°
    num_minibatches: int = 8            # 2048 / 256 = 8
    
    # èª¿æ•´è¨“ç·´åƒæ•¸
    total_timesteps: int = 10000000     # é™ä½ç¸½è¨“ç·´æ­¥æ•¸
    init_lr: float = 3e-4               # ç•¥å¾®èª¿é«˜å­¸ç¿’ç‡
    final_lr: float = 1e-5
    update_epochs: int = 4              # å¢åŠ æ›´æ–°è¼ªæ•¸
    
    # Transformer åƒæ•¸
    trxl_memory_length: int = 64        # é©åˆæ©Ÿå™¨äººä»»å‹™çš„è¨˜æ†¶é•·åº¦
    trxl_dim: int = 256                 # é™ä½ç¶­åº¦ç¯€çœè¨ˆç®—
    trxl_num_layers: int = 2            # æ¸›å°‘å±¤æ•¸
    
    # Webots å°ˆç”¨åƒæ•¸
    max_episode_steps: int = 2000       # episode æœ€å¤§æ­¥æ•¸
    save_interval: int = 100            # æ¨¡å‹ä¿å­˜é–“éš”(iterations)
    eval_interval: int = 50             # è©•ä¼°é–“éš”(iterations)
    webots_mode: str = "training"       # "training" æˆ– "evaluation"


class WebotsTrainer:
    """Webots ç’°å¢ƒä¸‹çš„ TrXL-PPO è¨“ç·´å™¨"""
    
    def __init__(self, args):
        self.args = args
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device("cuda" if args.cuda and torch.cuda.is_available() else "cpu")
        torch.set_default_device(self.device)
        
        # å‰µå»ºç’°å¢ƒ
        self.env = HexapodBalanceEnv(max_episode_steps=args.max_episode_steps)
        
        # æª¢æŸ¥ç’°å¢ƒå…¼å®¹æ€§
        self._validate_environment()
        
        # å‰µå»ºæ™ºèƒ½é«”
        if hasattr(self.env.action_space, 'shape'):
            action_space_shape = self.env.action_space.shape
        else:
            action_space_shape = (self.env.action_space.n,)

        self.agent = Agent(
            args, 
            self.env.observation_space, 
            action_space_shape,
            args.max_episode_steps
        ).to(self.device)
        
        # å‰µå»ºå„ªåŒ–å™¨
        self.optimizer = optim.AdamW(self.agent.parameters(), lr=args.init_lr)
        
        # è¨“ç·´ç‹€æ…‹
        self.global_step = 0
        self.iteration = 0
        self.episode_count = 0
        
        # ç›£æ§çµ±è¨ˆ
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.episode_infos = deque(maxlen=100)
        
        # TensorBoard
        run_name = f"webots_hexapod_trxl_ppo_{int(time.time())}"
        self.writer = SummaryWriter(f"runs/{run_name}")
        
        # è¨˜æ†¶é«”ç›¸é—œ
        self.current_memory = torch.zeros(
            (1, args.max_episode_steps, args.trxl_num_layers, args.trxl_dim), 
            dtype=torch.float32
        )
        self.current_episode_step = 0
        
        print("ğŸš€ Webots TrXL-PPO è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š ç’°å¢ƒ: {self.env.observation_space} -> {self.env.action_space}")
        print(f"ğŸ§  æ™ºèƒ½é«”: {args.trxl_dim}d Transformer, {args.trxl_num_layers} layers")
        print(f"ğŸ’¾ è¨­å‚™: {self.device}")
        print(f"ğŸ“ˆ TensorBoard: runs/{run_name}")

    def _validate_environment(self):
        """é©—è­‰ç’°å¢ƒå…¼å®¹æ€§"""
        # æª¢æŸ¥è§€æ¸¬ç©ºé–“
        assert len(self.env.observation_space.shape) == 1, "éœ€è¦å‘é‡è§€æ¸¬ç©ºé–“"
        
        # æª¢æŸ¥å‹•ä½œç©ºé–“  
        assert hasattr(self.env.action_space, 'shape'), "éœ€è¦é€£çºŒå‹•ä½œç©ºé–“"
        
        # æ¸¬è©¦ç’°å¢ƒ
        obs, info = self.env.reset()
        assert obs.shape == self.env.observation_space.shape, "è§€æ¸¬ç¶­åº¦ä¸åŒ¹é…"
        
        # æ¸¬è©¦å‹•ä½œ
        test_action = self.env.action_space.sample()
        obs, reward, terminated, truncated, info = self.env.step(test_action)
        
        print("âœ… ç’°å¢ƒå…¼å®¹æ€§é©—è­‰é€šé")

    def collect_rollout(self):
        """æ”¶é›†ä¸€å€‹ rollout çš„ç¶“é©—"""
        # å„²å­˜ rollout æ•¸æ“š
        observations = torch.zeros((self.args.num_steps, 1) + self.env.observation_space.shape)
        actions = torch.zeros((self.args.num_steps, 1, self.env.action_space.shape[0]))
        rewards = torch.zeros((self.args.num_steps, 1))
        dones = torch.zeros((self.args.num_steps, 1))
        log_probs = torch.zeros((self.args.num_steps, 1, self.env.action_space.shape[0]))
        values = torch.zeros((self.args.num_steps, 1))
        
        # ç°¡åŒ–çš„è¨˜æ†¶é«”è™•ç†
        stored_memories = []
        stored_memory_masks = torch.zeros((self.args.num_steps, 1, self.args.trxl_memory_length), dtype=torch.bool)
        stored_memory_indices = torch.zeros((self.args.num_steps, 1, self.args.trxl_memory_length), dtype=torch.long)
        
        # ç”Ÿæˆç°¡åŒ–çš„è¨˜æ†¶é«”é®ç½© (åªéœ€è¦ä¸€æ¬¡)
        memory_mask = torch.tril(torch.ones((self.args.trxl_memory_length, self.args.trxl_memory_length)), diagonal=-1)
        # ç²å–ç•¶å‰è§€æ¸¬
        if not hasattr(self, 'current_obs'):
            obs, info = self.env.reset()
            self.current_obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
            self.current_done = torch.tensor([False])
            self.current_episode_step = 0
            self.current_memory = torch.zeros(
                (1, self.args.max_episode_steps, self.args.trxl_num_layers, self.args.trxl_dim)
            )
        
        # æ”¶é›†ç¶“é©—
        for step in range(self.args.num_steps):
            self.global_step += 1
            
            # å„²å­˜ç•¶å‰è§€æ¸¬å’Œdone
            observations[step] = self.current_obs
            dones[step] = self.current_done.float()
            
            # æº–å‚™è¨˜æ†¶é«”çª—å£
            episode_step = min(self.current_episode_step, len(memory_indices) - 1)
            stored_memory_masks[step] = memory_mask[
                min(episode_step, self.args.trxl_memory_length - 1)
            ].unsqueeze(0)
            stored_memory_indices[step] = memory_indices[episode_step].unsqueeze(0)
            
            # æå–è¨˜æ†¶é«”çª—å£
            memory_window_indices = stored_memory_indices[step][0]  # [memory_length]
            # ç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
            valid_indices = torch.clamp(memory_window_indices, 0, self.current_memory.shape[1] - 1)
            memory_window = self.current_memory[0, valid_indices].unsqueeze(0)  # [1, memory_length, num_layers, dim]
            
            # æ™ºèƒ½é«”æ±ºç­–
            with torch.no_grad():
                action, logprob, _, value, new_memory = self.agent.get_action_and_value(
                    self.current_obs,
                    memory_window,
                    stored_memory_masks[step],
                    stored_memory_indices[step]
                )
                
                # æ›´æ–°è¨˜æ†¶é«”
                if self.current_episode_step < self.current_memory.shape[1]:
                    self.current_memory[0, self.current_episode_step] = new_memory[0]
                
                # å„²å­˜æ•¸æ“š
                actions[step] = action
                log_probs[step] = logprob
                values[step] = value
            
            # åŸ·è¡Œå‹•ä½œ
            obs, reward, terminated, truncated, info = self.env.step(action[0].cpu().numpy())
            done = terminated or truncated
            
            # æ›´æ–°ç‹€æ…‹
            rewards[step] = torch.tensor([reward])
            self.current_obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
            self.current_done = torch.tensor([done])
            
            # Episode ç®¡ç†
            if done:
                # è¨˜éŒ„ episode çµæœ
                self.episode_count += 1
                episode_info = {
                    'episode_reward': sum([rewards[i].item() for i in range(max(0, step-self.current_episode_step), step+1)]),
                    'episode_length': self.current_episode_step + 1,
                    'reason': info.get('reason', ''),
                    'stability_reward': info.get('stability_reward', 0),
                    'penalty': info.get('penalty', 0)
                }
                self.episode_infos.append(episode_info)
                
                # ä¿å­˜ç•¶å‰è¨˜æ†¶é«”
                stored_memories.append(self.current_memory[0].clone())
                
                # é‡ç½®ç’°å¢ƒ
                obs, info = self.env.reset()
                self.current_obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
                self.current_done = torch.tensor([False])
                self.current_episode_step = 0
                self.current_memory = torch.zeros(
                    (1, self.args.max_episode_steps, self.args.trxl_num_layers, self.args.trxl_dim)
                )
            else:
                self.current_episode_step += 1
            
            # ç‚ºä¸‹ä¸€æ­¥æº–å‚™è¨˜æ†¶é«”å¼•ç”¨
            if step == 0 or done:
                stored_memories.append(self.current_memory[0].clone())
        
        # ç¢ºä¿è¨˜æ†¶é«”åˆ—è¡¨é•·åº¦æ­£ç¢º
        while len(stored_memories) < self.args.num_steps:
            stored_memories.append(self.current_memory[0].clone())
        
        stored_memories = stored_memories[:self.args.num_steps]
        
        return {
            'observations': observations,
            'actions': actions,
            'rewards': rewards,
            'dones': dones,
            'log_probs': log_probs,
            'values': values,
            'stored_memories': torch.stack(stored_memories),
            'stored_memory_masks': stored_memory_masks,
            'stored_memory_indices': stored_memory_indices
        }

    def compute_advantages(self, rollout_data):
        """è¨ˆç®— GAE advantages"""
        rewards = rollout_data['rewards']
        values = rollout_data['values']
        dones = rollout_data['dones']
        
        # è¨ˆç®—ä¸‹ä¸€å€‹å€¼
        with torch.no_grad():
            memory_window_indices = rollout_data['stored_memory_indices'][-1]
            # å®‰å…¨çš„è¨˜æ†¶é«”ç´¢å¼•æå–
            max_valid_idx = self.current_memory.shape[1] - 1
            safe_indices = torch.clamp(memory_window_indices[0], 0, max_valid_idx)
            memory_window = self.current_memory[:, safe_indices]
            
            next_value = self.agent.get_value(
                self.current_obs,
                memory_window,
                rollout_data['stored_memory_masks'][-1],
                memory_window_indices
            )
        
        # GAE è¨ˆç®—
        advantages = torch.zeros_like(rewards)
        lastgaelam = 0
        
        for t in reversed(range(self.args.num_steps)):
            if t == self.args.num_steps - 1:
                nextnonterminal = 1.0 - self.current_done.float()
                nextvalues = next_value
            else:
                nextnonterminal = 1.0 - dones[t + 1]
                nextvalues = values[t + 1]
            
            delta = rewards[t] + self.args.gamma * nextvalues * nextnonterminal - values[t]
            advantages[t] = lastgaelam = delta + self.args.gamma * self.args.gae_lambda * nextnonterminal * lastgaelam
        
        returns = advantages + values
        
        return advantages, returns

    def update_policy(self, rollout_data, advantages, returns):
        """æ›´æ–°ç­–ç•¥"""
        # å±•å¹³æ•¸æ“š
        b_obs = rollout_data['observations'].reshape(-1, *rollout_data['observations'].shape[2:])
        b_logprobs = rollout_data['log_probs'].reshape(-1, *rollout_data['log_probs'].shape[2:])
        b_actions = rollout_data['actions'].reshape(-1, *rollout_data['actions'].shape[2:])
        b_advantages = advantages.reshape(-1)
        b_returns = returns.reshape(-1)
        b_values = rollout_data['values'].reshape(-1)
        b_memory_masks = rollout_data['stored_memory_masks'].reshape(-1, *rollout_data['stored_memory_masks'].shape[2:])
        b_memory_indices = rollout_data['stored_memory_indices'].reshape(-1, *rollout_data['stored_memory_indices'].shape[2:])
        stored_memories = rollout_data['stored_memories']
        
        # å­¸ç¿’ç‡é€€ç«
        frac = 1.0 - (self.global_step - self.args.num_steps) / self.args.total_timesteps
        frac = max(0.0, frac)  # ç¢ºä¿ä¸æœƒæ˜¯è² æ•¸
        lr = frac * self.args.init_lr + (1 - frac) * self.args.final_lr
        for param_group in self.optimizer.param_groups:
            param_group["lr"] = lr
        
        # ç†µç³»æ•¸é€€ç«
        ent_coef = frac * self.args.init_ent_coef + (1 - frac) * self.args.final_ent_coef
        
        clipfracs = []
        
        for epoch in range(self.args.update_epochs):
            b_inds = torch.randperm(self.args.batch_size)
            
            for start in range(0, self.args.batch_size, self.args.minibatch_size):
                end = start + self.args.minibatch_size
                mb_inds = b_inds[start:end]
                
                # ä¿®å¾©è¨˜æ†¶é«”çª—å£ç´¢å¼•å•é¡Œ
                try:
                    # ç²å–è¨˜æ†¶é«”ç´¢å¼• - éœ€è¦ç¢ºä¿å½¢ç‹€åŒ¹é…
                    mb_memory_indices = b_memory_indices[mb_inds]  # [minibatch_size, memory_length]
                    
                    # å‰µå»ºè¨˜æ†¶é«”çª—å£
                    batch_size = len(mb_inds)
                    memory_length = mb_memory_indices.shape[1]
                    embed_dim = stored_memories.shape[-1]
                    num_layers = stored_memories.shape[-2]
                    
                    # ç‚ºæ¯å€‹ minibatch æ¨£æœ¬å‰µå»ºè¨˜æ†¶é«”çª—å£
                    mb_memory_windows = torch.zeros(
                        batch_size, memory_length, num_layers, embed_dim,
                        device=stored_memories.device, dtype=stored_memories.dtype
                    )
                    
                    # é€å€‹æ¨£æœ¬è™•ç†è¨˜æ†¶é«”çª—å£
                    for i, sample_idx in enumerate(mb_inds):
                        # è¨ˆç®—é€™å€‹æ¨£æœ¬ä¾†è‡ªå“ªå€‹ step
                        step_idx = sample_idx // 1  # å› ç‚ºæ˜¯å–®ç’°å¢ƒï¼Œæ¯å€‹stepåªæœ‰1å€‹æ¨£æœ¬
                        
                        # ç¢ºä¿ step_idx åœ¨æœ‰æ•ˆç¯„åœå…§
                        step_idx = min(step_idx, stored_memories.shape[0] - 1)
                        
                        # ç²å–è©²æ­¥é©Ÿçš„è¨˜æ†¶é«”
                        step_memory = stored_memories[step_idx]  # [max_episode_steps, num_layers, embed_dim]
                        
                        # ç²å–è¨˜æ†¶é«”ç´¢å¼•
                        indices = mb_memory_indices[i]  # [memory_length]
                        
                        # ç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
                        indices = torch.clamp(indices, 0, step_memory.shape[0] - 1)
                        
                        # æå–è¨˜æ†¶é«”çª—å£
                        mb_memory_windows[i] = step_memory[indices]  # [memory_length, num_layers, embed_dim]
                    
                except Exception as e:
                    print(f"è¨˜æ†¶é«”çª—å£è™•ç†éŒ¯èª¤: {e}")
                    # ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼šå‰µå»ºé›¶è¨˜æ†¶é«”
                    batch_size = len(mb_inds)
                    memory_length = self.args.trxl_memory_length
                    mb_memory_windows = torch.zeros(
                        batch_size, memory_length, self.args.trxl_num_layers, self.args.trxl_dim,
                        device=self.device
                    )
                
                # å‰å‘å‚³æ’­
                _, newlogprob, entropy, newvalue, _ = self.agent.get_action_and_value(
                    b_obs[mb_inds], 
                    mb_memory_windows,
                    b_memory_masks[mb_inds], 
                    b_memory_indices[mb_inds], 
                    b_actions[mb_inds]
                )
                
                # ç­–ç•¥æå¤±
                mb_advantages = b_advantages[mb_inds]
                if self.args.norm_adv:
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
                
                logratio = newlogprob - b_logprobs[mb_inds]
                ratio = logratio.exp()

                # è™•ç†é€£çºŒå‹•ä½œçš„å½¢ç‹€
                if len(mb_advantages.shape) == 1:
                    mb_advantages = mb_advantages.unsqueeze(1)
                if len(ratio.shape) == 2 and ratio.shape[1] == 1:
                    mb_advantages = mb_advantages.expand_as(ratio)

                # PPO æˆªæ–·
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * torch.clamp(
                    ratio, 1 - self.args.clip_coef, 1 + self.args.clip_coef
                )
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
                
                # åƒ¹å€¼æå¤±
                v_loss = ((newvalue - b_returns[mb_inds]) ** 2).mean()
                
                # ç†µæå¤±
                entropy_loss = entropy.mean()
                
                # ç¸½æå¤±
                loss = pg_loss - ent_coef * entropy_loss + self.args.vf_coef * v_loss
                
                # åå‘å‚³æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
                self.optimizer.step()
                
                # è¨˜éŒ„çµ±è¨ˆ
                with torch.no_grad():
                    clipfracs += [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]
        
        return {
            'policy_loss': pg_loss.item(),
            'value_loss': v_loss.item(),
            'entropy_loss': entropy_loss.item(),
            'clipfrac': np.mean(clipfracs),
            'learning_rate': lr,
            'entropy_coef': ent_coef
        }

    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        start_time = time.time()
        
        num_iterations = self.args.total_timesteps // self.args.batch_size
        
        for iteration in range(1, num_iterations + 1):
            self.iteration = iteration
            
            # æ”¶é›†ç¶“é©—
            rollout_data = self.collect_rollout()
            
            # è¨ˆç®— advantages
            advantages, returns = self.compute_advantages(rollout_data)
            
            # æ›´æ–°ç­–ç•¥
            update_info = self.update_policy(rollout_data, advantages, returns)
            
            # è¨˜éŒ„çµ±è¨ˆ
            self._log_training_stats(update_info, start_time)
            
            # ä¿å­˜æª¢æŸ¥é»
            if iteration % self.args.save_interval == 0:
                self._save_checkpoint(iteration)
            
            # è©•ä¼°
            if iteration % self.args.eval_interval == 0:
                self._evaluate()
        
        print("âœ… è¨“ç·´å®Œæˆ!")
        self._save_checkpoint(iteration, final=True)

    def _log_training_stats(self, update_info, start_time):
        """è¨˜éŒ„è¨“ç·´çµ±è¨ˆ"""
        if len(self.episode_infos) > 0:
            # Episode çµ±è¨ˆ
            episode_rewards = [info['episode_reward'] for info in self.episode_infos]
            episode_lengths = [info['episode_length'] for info in self.episode_infos]
            
            avg_reward = np.mean(episode_rewards[-10:]) if episode_rewards else 0
            avg_length = np.mean(episode_lengths[-10:]) if episode_lengths else 0
            
            # æ‰“å°é€²åº¦
            sps = int(self.global_step / (time.time() - start_time))
            print(f"è¿­ä»£ {self.iteration:4d} | æ­¥æ•¸ {self.global_step:8d} | "
                  f"çå‹µ {avg_reward:6.2f} | é•·åº¦ {avg_length:4.0f} | "
                  f"SPS {sps:4d} | ç­–ç•¥æå¤± {update_info['policy_loss']:.3f}")
            
            # TensorBoard è¨˜éŒ„
            self.writer.add_scalar("episode/reward_mean", avg_reward, self.global_step)
            self.writer.add_scalar("episode/length_mean", avg_length, self.global_step)
            self.writer.add_scalar("episode/count", len(self.episode_infos), self.global_step)
            
        # è¨“ç·´çµ±è¨ˆ
        for key, value in update_info.items():
            self.writer.add_scalar(f"training/{key}", value, self.global_step)
        
        self.writer.add_scalar("performance/sps", sps, self.global_step)

    def _save_checkpoint(self, iteration, final=False):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'iteration': iteration,
            'global_step': self.global_step,
            'agent_state_dict': self.agent.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'args': self.args
        }
        
        filename = f"checkpoint_final.pt" if final else f"checkpoint_{iteration}.pt"
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {filename}")

    def _evaluate(self):
        """è©•ä¼°æ™ºèƒ½é«”æ€§èƒ½"""
        print("ğŸ” é€²è¡Œè©•ä¼°...")
        eval_episodes = 5
        eval_rewards = []
        eval_lengths = []
        
        for _ in range(eval_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            # é‡ç½®è©•ä¼°è¨˜æ†¶é«”
            eval_memory = torch.zeros(
                (1, self.args.max_episode_steps, self.args.trxl_num_layers, self.args.trxl_dim)
            )
            eval_step = 0
            
            while not done and episode_length < self.args.max_episode_steps:
                obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)
                
                # æº–å‚™è¨˜æ†¶é«”çª—å£
                memory_indices = torch.arange(
                    max(0, eval_step - self.args.trxl_memory_length + 1),
                    eval_step + 1
                ).unsqueeze(0)
                
                if len(memory_indices[0]) < self.args.trxl_memory_length:
                    padding = self.args.trxl_memory_length - len(memory_indices[0])
                    memory_indices = torch.cat([
                        torch.zeros(1, padding, dtype=torch.long),
                        memory_indices
                    ], dim=1)
                
                memory_window = eval_memory[:, memory_indices[0]]
                memory_mask = torch.tril(torch.ones((self.args.trxl_memory_length, self.args.trxl_memory_length)), diagonal=-1)
                memory_mask = memory_mask[min(eval_step, self.args.trxl_memory_length - 1)].unsqueeze(0)
                
                with torch.no_grad():
                    action, _, _, _, new_memory = self.agent.get_action_and_value(
                        obs_tensor, memory_window, memory_mask, memory_indices
                    )
                    eval_memory[0, eval_step] = new_memory[0]
                
                obs, reward, terminated, truncated, info = self.env.step(action[0].cpu().numpy())
                done = terminated or truncated
                episode_reward += reward
                episode_length += 1
                eval_step += 1
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
        
        avg_eval_reward = np.mean(eval_rewards)
        avg_eval_length = np.mean(eval_lengths)
        
        print(f"ğŸ“Š è©•ä¼°çµæœ: å¹³å‡çå‹µ {avg_eval_reward:.2f}, å¹³å‡é•·åº¦ {avg_eval_length:.1f}")
        
        self.writer.add_scalar("evaluation/reward_mean", avg_eval_reward, self.global_step)
        self.writer.add_scalar("evaluation/length_mean", avg_eval_length, self.global_step)


def main():
    """ä¸»å‡½æ•¸"""
    # è§£æåƒæ•¸
    args = tyro.cli(WebotsTrXLArgs)
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)
    
    # å‰µå»ºä¸¦é‹è¡Œè¨“ç·´å™¨
    trainer = WebotsTrainer(args)
    
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\nâš ï¸  è¨“ç·´è¢«ä¸­æ–·")
        trainer._save_checkpoint(trainer.iteration, final=True)
    except Exception as e:
        print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
        raise
    finally:
        trainer.writer.close()
        trainer.env.close()


if __name__ == "__main__":
    main()