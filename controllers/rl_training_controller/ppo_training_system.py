import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import time
from collections import deque
from dataclasses import dataclass
from torch.utils.tensorboard import SummaryWriter
import json
import pickle

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  Weights & Biases æœªå®‰è£ï¼Œåƒ…ä½¿ç”¨ TensorBoard")


@dataclass
class PPOConfig:
    """PPOè¨“ç·´é…ç½®"""
    # ç’°å¢ƒé…ç½®
    max_episode_steps: int = 2000
    sequence_length: int = 50
    
    # ç¶²è·¯é…ç½®
    hidden_size: int = 128
    n_layer: int = 3
    n_head: int = 2
    dropout: float = 0.1
    action_range: float = 1.0
    
    # PPOé…ç½®
    learning_rate: float = 3e-4
    clip_coef: float = 0.2
    value_loss_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5
    
    # è¨“ç·´é…ç½®
    total_timesteps: int = 1000000
    episodes_per_update: int = 4    # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„episodeæ•¸
    update_epochs: int = 4          # æ¯æ¬¡æ•¸æ“šçš„æ›´æ–°epochs
    gae_lambda: float = 0.95
    gamma: float = 0.99
    
    # å­¸ç¿’ç‡èª¿åº¦
    anneal_lr: bool = True
    
    # æ—©åœå’Œä¿å­˜
    target_reward: float = 0.8      # ç›®æ¨™å¹³å‡çå‹µ
    save_frequency: int = 50        # æ¯50æ¬¡æ›´æ–°ä¿å­˜ä¸€æ¬¡
    eval_frequency: int = 20        # æ¯20æ¬¡æ›´æ–°è©•ä¼°ä¸€æ¬¡
    
    # æ—¥èªŒé…ç½®
    log_frequency: int = 10         # æ¯10æ¬¡æ›´æ–°è¨˜éŒ„ä¸€æ¬¡
    use_wandb: bool = False
    project_name: str = "hexapod_balance"
    run_name: str = "transformer_ppo"


class GAE:
    """Generalized Advantage Estimation"""
    
    def __init__(self, gamma=0.99, gae_lambda=0.95):
        self.gamma = gamma
        self.gae_lambda = gae_lambda
    
    def compute_advantages(self, rewards, values, dones, next_value):
        """
        è¨ˆç®—GAEå„ªå‹¢å‡½æ•¸
        
        Args:
            rewards: [seq_len] çå‹µåºåˆ—
            values: [seq_len] åƒ¹å€¼åºåˆ—  
            dones: [seq_len] çµ‚æ­¢æ¨™èªŒ
            next_value: scalar ä¸‹ä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼
        
        Returns:
            advantages: [seq_len] å„ªå‹¢å‡½æ•¸
            returns: [seq_len] å›å ±
        """
        seq_len = len(rewards)
        advantages = torch.zeros(seq_len)
        returns = torch.zeros(seq_len)
        
        # å¾å¾Œå¾€å‰è¨ˆç®—
        gae = 0
        for t in reversed(range(seq_len)):
            if t == seq_len - 1:
                next_non_terminal = 1.0 - dones[t]
                next_val = next_value
            else:
                next_non_terminal = 1.0 - dones[t + 1]
                next_val = values[t + 1]
            
            # TD error
            delta = rewards[t] + self.gamma * next_val * next_non_terminal - values[t]
            
            # GAE
            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae
            advantages[t] = gae
            
            # Return
            returns[t] = advantages[t] + values[t]
        
        return advantages, returns


class ExperienceBuffer:
    """ç¶“é©—ç·©è¡å€"""
    
    def __init__(self, config):
        self.config = config
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç·©è¡å€"""
        self.states_seq = []      # ç‹€æ…‹åºåˆ—
        self.actions_seq = []     # å‹•ä½œåºåˆ—
        self.rewards_seq = []     # çå‹µåºåˆ—
        self.values_seq = []      # åƒ¹å€¼åºåˆ—
        self.log_probs_seq = []   # å°æ•¸æ¦‚ç‡åºåˆ—
        self.dones_seq = []       # çµ‚æ­¢æ¨™èªŒåºåˆ—
        
        # Episodeç´šåˆ¥çš„æ•¸æ“š
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_infos = []
    
    def add_episode(self, episode_data):
        """æ·»åŠ ä¸€å€‹å®Œæ•´çš„episode"""
        self.states_seq.append(episode_data['states'])
        self.actions_seq.append(episode_data['actions'])
        self.rewards_seq.append(episode_data['rewards'])
        self.values_seq.append(episode_data['values'])
        self.log_probs_seq.append(episode_data['log_probs'])
        self.dones_seq.append(episode_data['dones'])
        
        # Episodeçµ±è¨ˆ
        self.episode_rewards.append(episode_data['total_reward'])
        self.episode_lengths.append(episode_data['length'])
        self.episode_infos.append(episode_data['info'])
    
    def get_batch_data(self):
        """ç²å–æ‰¹æ¬¡è¨“ç·´æ•¸æ“š"""
        if len(self.states_seq) == 0:
            return None
            
        return {
            'states_seq': self.states_seq,
            'actions_seq': self.actions_seq,
            'rewards_seq': self.rewards_seq,
            'values_seq': self.values_seq,
            'log_probs_seq': self.log_probs_seq,
            'dones_seq': self.dones_seq,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'episode_infos': self.episode_infos
        }
    
    def size(self):
        """è¿”å›ç·©è¡å€ä¸­çš„episodeæ•¸é‡"""
        return len(self.states_seq)


class PPOTrainer:
    """PPOè¨“ç·´å™¨"""
    
    def __init__(self, env, policy, config):
        self.env = env
        self.policy = policy
        self.config = config
        
        # è¨“ç·´çµ„ä»¶
        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.learning_rate)
        self.gae = GAE(gamma=config.gamma, gae_lambda=config.gae_lambda)
        self.experience_buffer = ExperienceBuffer(config)
        
        # è¨“ç·´ç‹€æ…‹
        self.global_step = 0
        self.update_count = 0
        self.best_avg_reward = float('-inf')
        
        # è¨­ç½®æ—¥èªŒ
        self.setup_logging()
        
        # ç­–ç•¥åŒ…è£å™¨ï¼ˆç”¨æ–¼ç’°å¢ƒäº¤äº’ï¼‰
        from transformer_policy import TransformerPolicyWrapper
        self.policy_wrapper = TransformerPolicyWrapper(self.policy)
        
        print(f"âœ… PPOè¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š é…ç½®: {self.config}")
    
    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir = f"runs/{self.config.run_name}_{int(time.time())}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.output_dir)
        
        # Weights & Biases
        if self.config.use_wandb and WANDB_AVAILABLE:
            wandb.init(
                project=self.config.project_name,
                name=self.config.run_name,
                config=self.config.__dict__
            )
            self.use_wandb = True
        else:
            self.use_wandb = False
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.output_dir, "config.json")
        with open(config_path, 'w') as f:
            json.dump(self.config.__dict__, f, indent=2)
        
        print(f"ğŸ“ æ—¥èªŒä¿å­˜åˆ°: {self.output_dir}")
    
    def collect_episode(self):
        """æ”¶é›†ä¸€å€‹å®Œæ•´çš„episode"""
        # é‡ç½®ç’°å¢ƒå’Œç­–ç•¥åŒ…è£å™¨
        state = self.env.reset()
        self.policy_wrapper.reset_sequence_cache()
        
        # Episodeæ•¸æ“š
        states = []
        actions = []
        rewards = []
        values = []
        log_probs = []
        dones = []
        
        total_reward = 0
        step_count = 0
        episode_info = {}
        
        while True:
            # ç²å–ç­–ç•¥è¼¸å‡º
            self.policy.eval()
            with torch.no_grad():
                # ç²å–ç•¶å‰åºåˆ—æ•¸æ“š
                seq_data = self.policy_wrapper.get_sequence_data()
                
                # ç²å–å‹•ä½œå’Œåƒ¹å€¼
                action, log_prob, entropy, value = self.policy.get_action_and_value(
                    seq_data['states'], 
                    seq_data['actions'], 
                    seq_data['rewards']
                )
                
                action = action.cpu().numpy()
                log_prob = log_prob.cpu().item()
                value = value.cpu().item()
            
            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # è¨˜éŒ„æ•¸æ“š
            states.append(state.copy())
            actions.append(action.copy())
            rewards.append(reward)
            values.append(value)
            log_probs.append(log_prob)
            dones.append(done)
            
            # æ›´æ–°ç­–ç•¥åŒ…è£å™¨
            self.policy_wrapper.update_sequence(next_state, action, reward)
            
            # çµ±è¨ˆ
            total_reward += reward
            step_count += 1
            self.global_step += 1
            
            # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
            if done or step_count >= self.config.max_episode_steps:
                episode_info = {
                    'reason': info.get('reason', 'max_steps'),
                    'final_imu': info.get('imu_data', (0, 0)),
                    'final_gps': info.get('gps_data', (0, 0, 0))
                }
                break
            
            state = next_state
        
        # è½‰æ›ç‚ºå¼µé‡
        episode_data = {
            'states': torch.tensor(np.array(states), dtype=torch.float32),
            'actions': torch.tensor(np.array(actions), dtype=torch.float32),
            'rewards': torch.tensor(rewards, dtype=torch.float32),
            'values': torch.tensor(values, dtype=torch.float32),
            'log_probs': torch.tensor(log_probs, dtype=torch.float32),
            'dones': torch.tensor(dones, dtype=torch.float32),
            'total_reward': total_reward,
            'length': step_count,
            'info': episode_info
        }
        
        return episode_data
    
    def collect_experiences(self):
        """æ”¶é›†è¨“ç·´æ•¸æ“š"""
        self.experience_buffer.clear()
        
        for episode_idx in range(self.config.episodes_per_update):
            episode_data = self.collect_episode()
            self.experience_buffer.add_episode(episode_data)
            
            # ç°¡å–®é€²åº¦é¡¯ç¤º
            if episode_idx % max(1, self.config.episodes_per_update // 4) == 0:
                print(f"  æ”¶é›†episode {episode_idx+1}/{self.config.episodes_per_update}, "
                      f"çå‹µ: {episode_data['total_reward']:.3f}, "
                      f"é•·åº¦: {episode_data['length']}")
        
        return self.experience_buffer.get_batch_data()
    
    def compute_advantages_and_returns(self, batch_data):
        """è¨ˆç®—æ‰€æœ‰episodeçš„å„ªå‹¢å‡½æ•¸å’Œå›å ±"""
        all_advantages = []
        all_returns = []
        
        for i in range(len(batch_data['states_seq'])):
            rewards = batch_data['rewards_seq'][i]
            values = batch_data['values_seq'][i]
            dones = batch_data['dones_seq'][i]
            
            # è¨ˆç®—ä¸‹ä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼ï¼ˆç”¨æ–¼bootstrapï¼‰
            if dones[-1]:
                next_value = 0.0  # EpisodeçµæŸï¼Œä¸‹ä¸€å€‹ç‹€æ…‹åƒ¹å€¼ç‚º0
            else:
                # EpisodeæœªçµæŸï¼Œä¼°è¨ˆä¸‹ä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼
                # é€™è£¡ç°¡åŒ–ç‚ºä½¿ç”¨æœ€å¾Œä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼
                next_value = values[-1].item()
            
            # è¨ˆç®—GAE
            advantages, returns = self.gae.compute_advantages(rewards, values, dones, next_value)
            
            all_advantages.append(advantages)
            all_returns.append(returns)
        
        return all_advantages, all_returns
    
    def ppo_update(self, batch_data):
        """åŸ·è¡ŒPPOæ›´æ–°"""
        # è¨ˆç®—å„ªå‹¢å‡½æ•¸å’Œå›å ±
        all_advantages, all_returns = self.compute_advantages_and_returns(batch_data)
        
        # æº–å‚™è¨“ç·´æ•¸æ“š
        train_data = []
        for i in range(len(batch_data['states_seq'])):
            # æ§‹å»ºåºåˆ—æ•¸æ“š
            seq_len = len(batch_data['states_seq'][i])
            
            # å‰µå»ºå¡«å……çš„åºåˆ—ï¼ˆç¢ºä¿é•·åº¦ç‚ºsequence_lengthï¼‰
            padded_states = torch.zeros(self.config.sequence_length, 6)
            padded_actions = torch.zeros(self.config.sequence_length, 6)
            padded_rewards = torch.zeros(self.config.sequence_length)
            
            # å¡«å……å¯¦éš›æ•¸æ“š
            actual_len = min(seq_len, self.config.sequence_length)
            padded_states[:actual_len] = batch_data['states_seq'][i][:actual_len]
            padded_actions[:actual_len] = batch_data['actions_seq'][i][:actual_len]
            padded_rewards[:actual_len] = batch_data['rewards_seq'][i][:actual_len]
            
            # ç‚ºæ¯å€‹æ™‚é–“æ­¥å‰µå»ºè¨“ç·´æ•¸æ“š
            for t in range(actual_len):
                train_data.append({
                    'states_seq': padded_states,
                    'actions_seq': padded_actions,
                    'rewards_seq': padded_rewards,
                    'action': batch_data['actions_seq'][i][t],
                    'old_log_prob': batch_data['log_probs_seq'][i][t],
                    'advantage': all_advantages[i][t],
                    'return': all_returns[i][t],
                    'old_value': batch_data['values_seq'][i][t]
                })
        
        # æ¨™æº–åŒ–å„ªå‹¢å‡½æ•¸
        advantages = torch.tensor([data['advantage'] for data in train_data])
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        for i, data in enumerate(train_data):
            data['advantage'] = advantages[i]
        
        # å¤šæ¬¡æ›´æ–°
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        total_kl_div = 0
        
        for epoch in range(self.config.update_epochs):
            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            np.random.shuffle(train_data)
            
            epoch_policy_loss = 0
            epoch_value_loss = 0
            epoch_entropy_loss = 0
            epoch_kl_div = 0
            
            for data in train_data:
                self.policy.train()
                
                # å‰å‘å‚³æ’­
                action, log_prob, entropy, value = self.policy.get_action_and_value(
                    data['states_seq'],
                    data['actions_seq'],
                    data['rewards_seq'],
                    action=data['action']
                )
                
                # è¨ˆç®—æå¤±
                # 1. Policy Loss (PPO Clipped)
                ratio = torch.exp(log_prob - data['old_log_prob'])
                surr1 = data['advantage'] * ratio
                surr2 = data['advantage'] * torch.clamp(ratio, 1 - self.config.clip_coef, 1 + self.config.clip_coef)
                policy_loss = -torch.min(surr1, surr2)
                
                # 2. Value Loss
                value_loss = 0.5 * (value - data['return']) ** 2
                
                # 3. Entropy Loss
                entropy_loss = -entropy
                
                # 4. Total Loss
                loss = policy_loss + self.config.value_loss_coef * value_loss + self.config.entropy_coef * entropy_loss
                
                # åå‘å‚³æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)
                
                self.optimizer.step()
                
                # çµ±è¨ˆ
                epoch_policy_loss += policy_loss.item()
                epoch_value_loss += value_loss.item()
                epoch_entropy_loss += entropy_loss.item()
                
                # KLæ•£åº¦ï¼ˆç”¨æ–¼ç›£æ§ï¼‰
                kl_div = data['old_log_prob'] - log_prob
                epoch_kl_div += kl_div.item()
            
            # Epochçµ±è¨ˆ
            num_samples = len(train_data)
            total_policy_loss += epoch_policy_loss / num_samples
            total_value_loss += epoch_value_loss / num_samples
            total_entropy_loss += epoch_entropy_loss / num_samples
            total_kl_div += epoch_kl_div / num_samples
        
        # è¿”å›è¨“ç·´çµ±è¨ˆ
        num_epochs = self.config.update_epochs
        return {
            'policy_loss': total_policy_loss / num_epochs,
            'value_loss': total_value_loss / num_epochs,
            'entropy_loss': total_entropy_loss / num_epochs,
            'kl_div': total_kl_div / num_epochs,
            'num_samples': len(train_data)
        }
    
    def evaluate_policy(self, num_episodes=5):
        """è©•ä¼°ç­–ç•¥æ€§èƒ½"""
        print("ğŸ§ª è©•ä¼°ç­–ç•¥æ€§èƒ½...")
        
        eval_rewards = []
        eval_lengths = []
        
        for _ in range(num_episodes):
            episode_data = self.collect_episode()
            eval_rewards.append(episode_data['total_reward'])
            eval_lengths.append(episode_data['length'])
        
        avg_reward = np.mean(eval_rewards)
        avg_length = np.mean(eval_lengths)
        
        print(f"  è©•ä¼°çµæœ: å¹³å‡çå‹µ={avg_reward:.3f}, å¹³å‡é•·åº¦={avg_length:.1f}")
        
        return {
            'avg_reward': avg_reward,
            'avg_length': avg_length,
            'rewards': eval_rewards,
            'lengths': eval_lengths
        }
    
    def save_model(self, suffix=""):
        """ä¿å­˜æ¨¡å‹"""
        save_path = os.path.join(self.output_dir, f"model{suffix}.pt")
        
        save_data = {
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config.__dict__,
            'global_step': self.global_step,
            'update_count': self.update_count,
            'best_avg_reward': self.best_avg_reward
        }
        
        torch.save(save_data, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
        
        return save_path
    
    def load_model(self, checkpoint_path):
        """è¼‰å…¥æ¨¡å‹"""
        checkpoint = torch.load(checkpoint_path)
        
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.global_step = checkpoint['global_step']
        self.update_count = checkpoint['update_count']
        self.best_avg_reward = checkpoint['best_avg_reward']
        
        print(f"ğŸ“ æ¨¡å‹å·²è¼‰å…¥: {checkpoint_path}")
    
    def update_learning_rate(self):
        """æ›´æ–°å­¸ç¿’ç‡"""
        if self.config.anneal_lr:
            frac = 1.0 - self.global_step / self.config.total_timesteps
            new_lr = frac * self.config.learning_rate
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
    
    def log_metrics(self, metrics):
        """è¨˜éŒ„è¨“ç·´æŒ‡æ¨™"""
        # TensorBoard
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, self.update_count)
        
        # Weights & Biases
        if self.use_wandb:
            wandb.log(metrics, step=self.update_count)
    
    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        print(f"ç›®æ¨™: {self.config.total_timesteps:,} ç¸½æ­¥æ•¸")
        
        start_time = time.time()
        
        while self.global_step < self.config.total_timesteps:
            self.update_count += 1
            
            # æ”¶é›†ç¶“é©—
            print(f"\nğŸ“Š æ›´æ–° {self.update_count} - æ”¶é›†ç¶“é©—...")
            batch_data = self.collect_experiences()
            
            # PPOæ›´æ–°
            print("ğŸ”„ åŸ·è¡ŒPPOæ›´æ–°...")
            update_stats = self.ppo_update(batch_data)
            
            # æ›´æ–°å­¸ç¿’ç‡
            self.update_learning_rate()
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            avg_reward = np.mean(batch_data['episode_rewards'])
            avg_length = np.mean(batch_data['episode_lengths'])
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # æ—¥èªŒè¨˜éŒ„
            if self.update_count % self.config.log_frequency == 0:
                elapsed_time = time.time() - start_time
                steps_per_sec = self.global_step / elapsed_time
                
                metrics = {
                    'train/avg_reward': avg_reward,
                    'train/avg_length': avg_length,
                    'train/policy_loss': update_stats['policy_loss'],
                    'train/value_loss': update_stats['value_loss'],
                    'train/entropy_loss': update_stats['entropy_loss'],
                    'train/kl_div': update_stats['kl_div'],
                    'train/learning_rate': current_lr,
                    'train/global_step': self.global_step,
                    'train/steps_per_sec': steps_per_sec
                }
                
                self.log_metrics(metrics)
                
                print(f"ğŸ“ˆ æ›´æ–° {self.update_count}: çå‹µ={avg_reward:.3f}, "
                      f"é•·åº¦={avg_length:.1f}, SPS={steps_per_sec:.1f}")
            
            # å®šæœŸè©•ä¼°
            if self.update_count % self.config.eval_frequency == 0:
                eval_stats = self.evaluate_policy()
                
                eval_metrics = {
                    'eval/avg_reward': eval_stats['avg_reward'],
                    'eval/avg_length': eval_stats['avg_length']
                }
                self.log_metrics(eval_metrics)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_stats['avg_reward'] > self.best_avg_reward:
                    self.best_avg_reward = eval_stats['avg_reward']
                    self.save_model("_best")
            
            # å®šæœŸä¿å­˜
            if self.update_count % self.config.save_frequency == 0:
                self.save_model(f"_update_{self.update_count}")
            
            # æ—©åœæª¢æŸ¥
            if avg_reward >= self.config.target_reward:
                print(f"ğŸ¯ é”åˆ°ç›®æ¨™çå‹µ {self.config.target_reward}! è¨“ç·´å®Œæˆ!")
                break
        
        # è¨“ç·´å®Œæˆ
        print("\nâœ… è¨“ç·´å®Œæˆ!")
        self.save_model("_final")
        
        # æœ€çµ‚è©•ä¼°
        final_eval = self.evaluate_policy(num_episodes=10)
        print(f"ğŸ† æœ€çµ‚è©•ä¼°: å¹³å‡çå‹µ={final_eval['avg_reward']:.3f}")
        
        # æ¸…ç†
        self.writer.close()
        if self.use_wandb:
            wandb.finish()


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¤– å…­è¶³æ©Ÿå™¨äººå¹³è¡¡è¨“ç·´ - PPO + Transformer")
    
    # é…ç½®
    config = PPOConfig(
        # åŸºæœ¬é…ç½®
        total_timesteps=500000,
        episodes_per_update=4,
        update_epochs=4,
        
        # ç¶²è·¯é…ç½®
        hidden_size=128,
        n_layer=3,
        n_head=2,
        
        # PPOé…ç½®
        learning_rate=3e-4,
        clip_coef=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        
        # æ—¥èªŒé…ç½®
        use_wandb=False,  # è¨­ç‚ºTrueå•Ÿç”¨Weights & Biases
        run_name=f"hexapod_ppo_{int(time.time())}"
    )
    
    # å‰µå»ºç’°å¢ƒ
    print("ğŸŒ åˆå§‹åŒ–ç’°å¢ƒ...")
    from hexapod_balance_env import HexapodBalanceEnv
    env = HexapodBalanceEnv(
        max_episode_steps=config.max_episode_steps,
        sequence_length=config.sequence_length
    )
    
    # å‰µå»ºç­–ç•¥ç¶²è·¯
    print("ğŸ§  åˆå§‹åŒ–Transformerç­–ç•¥ç¶²è·¯...")
    from transformer_policy import TransformerPolicyNetwork
    policy = TransformerPolicyNetwork(
        state_dim=6,
        action_dim=6,
        sequence_length=config.sequence_length,
        hidden_size=config.hidden_size,
        n_layer=config.n_layer,
        n_head=config.n_head,
        dropout=config.dropout,
        action_range=config.action_range
    )
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = PPOTrainer(env, policy, config)
    
    # é–‹å§‹è¨“ç·´
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è¨“ç·´è¢«ä¸­æ–·")
        trainer.save_model("_interrupted")
    except Exception as e:
        print(f"\nâŒ è¨“ç·´å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        trainer.save_model("_error")
    finally:
        print("ğŸ§¹ æ¸…ç†è³‡æº...")
        env.close()


if __name__ == "__main__":
    main()