import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import time
from collections import deque
from dataclasses import dataclass
from torch.utils.tensorboard import SummaryWriter
import json
import pickle

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("‚ö†Ô∏è  Weights & Biases Êú™ÂÆâË£ùÔºåÂÉÖ‰ΩøÁî® TensorBoard")


@dataclass
class PPOConfig:
    """PPOË®ìÁ∑¥ÈÖçÁΩÆ"""
    # Áí∞Â¢ÉÈÖçÁΩÆ
    max_episode_steps: int = 2000
    sequence_length: int = 50
    
    # Á∂≤Ë∑ØÈÖçÁΩÆ
    hidden_size: int = 128
    n_layer: int = 3
    n_head: int = 2
    dropout: float = 0.1
    action_range: float = 1.0
    
    # PPOÈÖçÁΩÆ
    learning_rate: float = 3e-4
    clip_coef: float = 0.2
    value_loss_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5
    
    # Ë®ìÁ∑¥ÈÖçÁΩÆ
    total_timesteps: int = 1000000
    episodes_per_update: int = 4    # ÊØèÊ¨°Êõ¥Êñ∞Êî∂ÈõÜÁöÑepisodeÊï∏
    update_epochs: int = 4          # ÊØèÊ¨°Êï∏ÊìöÁöÑÊõ¥Êñ∞epochs
    gae_lambda: float = 0.95
    gamma: float = 0.99
    
    # Â≠∏ÁøíÁéáË™øÂ∫¶
    anneal_lr: bool = True
    
    # Êó©ÂÅúÂíå‰øùÂ≠ò
    target_reward: float = 0.8      # ÁõÆÊ®ôÂπ≥ÂùáÁçéÂãµ
    save_frequency: int = 50        # ÊØè50Ê¨°Êõ¥Êñ∞‰øùÂ≠ò‰∏ÄÊ¨°
    eval_frequency: int = 20        # ÊØè20Ê¨°Êõ¥Êñ∞Ë©ï‰º∞‰∏ÄÊ¨°
    
    # Êó•Ë™åÈÖçÁΩÆ
    log_frequency: int = 10         # ÊØè10Ê¨°Êõ¥Êñ∞Ë®òÈåÑ‰∏ÄÊ¨°
    use_wandb: bool = False
    project_name: str = "hexapod_balance"
    run_name: str = "transformer_ppo"


class GAE:
    """Generalized Advantage Estimation"""
    
    def __init__(self, gamma=0.99, gae_lambda=0.95):
        self.gamma = gamma
        self.gae_lambda = gae_lambda
    
    def compute_advantages(self, rewards, values, dones, next_value):
        """
        Ë®àÁÆóGAEÂÑ™Âã¢ÂáΩÊï∏
        
        Args:
            rewards: [seq_len] ÁçéÂãµÂ∫èÂàó
            values: [seq_len] ÂÉπÂÄºÂ∫èÂàó  
            dones: [seq_len] ÁµÇÊ≠¢Ê®ôË™å
            next_value: scalar ‰∏ã‰∏ÄÂÄãÁãÄÊÖãÁöÑÂÉπÂÄº
        
        Returns:
            advantages: [seq_len] ÂÑ™Âã¢ÂáΩÊï∏
            returns: [seq_len] ÂõûÂ†±
        """
        seq_len = len(rewards)
        advantages = torch.zeros(seq_len)
        returns = torch.zeros(seq_len)
        
        # ÂæûÂæåÂæÄÂâçË®àÁÆó
        gae = 0
        for t in reversed(range(seq_len)):
            if t == seq_len - 1:
                next_non_terminal = 1.0 - dones[t]
                next_val = next_value
            else:
                next_non_terminal = 1.0 - dones[t + 1]
                next_val = values[t + 1]
            
            # TD error
            delta = rewards[t] + self.gamma * next_val * next_non_terminal - values[t]
            
            # GAE
            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae
            advantages[t] = gae
            
            # Return
            returns[t] = advantages[t] + values[t]
        
        return advantages, returns


class ExperienceBuffer:
    """Á∂ìÈ©óÁ∑©Ë°ùÂçÄ"""
    
    def __init__(self, config):
        self.config = config
        self.clear()
    
    def clear(self):
        """Ê∏ÖÁ©∫Á∑©Ë°ùÂçÄ"""
        self.states_seq = []      # ÁãÄÊÖãÂ∫èÂàó
        self.actions_seq = []     # Âãï‰ΩúÂ∫èÂàó
        self.rewards_seq = []     # ÁçéÂãµÂ∫èÂàó
        self.values_seq = []      # ÂÉπÂÄºÂ∫èÂàó
        self.log_probs_seq = []   # Â∞çÊï∏Ê¶ÇÁéáÂ∫èÂàó
        self.dones_seq = []       # ÁµÇÊ≠¢Ê®ôË™åÂ∫èÂàó
        
        # EpisodeÁ¥öÂà•ÁöÑÊï∏Êìö
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_infos = []
    
    def add_episode(self, episode_data):
        """Ê∑ªÂä†‰∏ÄÂÄãÂÆåÊï¥ÁöÑepisode"""
        self.states_seq.append(episode_data['states'])
        self.actions_seq.append(episode_data['actions'])
        self.rewards_seq.append(episode_data['rewards'])
        self.values_seq.append(episode_data['values'])
        self.log_probs_seq.append(episode_data['log_probs'])
        self.dones_seq.append(episode_data['dones'])
        
        # EpisodeÁµ±Ë®à
        self.episode_rewards.append(episode_data['total_reward'])
        self.episode_lengths.append(episode_data['length'])
        self.episode_infos.append(episode_data['info'])
    
    def get_batch_data(self):
        """Áç≤ÂèñÊâπÊ¨°Ë®ìÁ∑¥Êï∏Êìö"""
        if len(self.states_seq) == 0:
            return None
            
        return {
            'states_seq': self.states_seq,
            'actions_seq': self.actions_seq,
            'rewards_seq': self.rewards_seq,
            'values_seq': self.values_seq,
            'log_probs_seq': self.log_probs_seq,
            'dones_seq': self.dones_seq,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'episode_infos': self.episode_infos
        }
    
    def size(self):
        """ËøîÂõûÁ∑©Ë°ùÂçÄ‰∏≠ÁöÑepisodeÊï∏Èáè"""
        return len(self.states_seq)


class PPOTrainer:
    """PPOË®ìÁ∑¥Âô®"""
    
    def __init__(self, env, policy, config):
        self.env = env
        self.policy = policy
        self.config = config
        
        # Ë®ìÁ∑¥ÁµÑ‰ª∂
        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.learning_rate)
        self.gae = GAE(gamma=config.gamma, gae_lambda=config.gae_lambda)
        self.experience_buffer = ExperienceBuffer(config)
        
        # Ë®ìÁ∑¥ÁãÄÊÖã
        self.global_step = 0
        self.update_count = 0
        self.best_avg_reward = float('-inf')
        
        # Ë®≠ÁΩÆÊó•Ë™å
        self.setup_logging()
        
        # Á≠ñÁï•ÂåÖË£ùÂô®ÔºàÁî®ÊñºÁí∞Â¢É‰∫§‰∫íÔºâ
        from transformer_policy import TransformerPolicyWrapper
        self.policy_wrapper = TransformerPolicyWrapper(self.policy)
        
        print(f"‚úÖ PPOË®ìÁ∑¥Âô®ÂàùÂßãÂåñÂÆåÊàê")
        print(f"üìä ÈÖçÁΩÆ: {self.config}")
    
    def setup_logging(self):
        """Ë®≠ÁΩÆÊó•Ë™åÁ≥ªÁµ±"""
        # ÂâµÂª∫Ëº∏Âá∫ÁõÆÈåÑ
        self.output_dir = f"runs/{self.config.run_name}_{int(time.time())}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.output_dir)
        
        # Weights & Biases
        if self.config.use_wandb and WANDB_AVAILABLE:
            wandb.init(
                project=self.config.project_name,
                name=self.config.run_name,
                config=self.config.__dict__
            )
            self.use_wandb = True
        else:
            self.use_wandb = False
        
        # ‰øùÂ≠òÈÖçÁΩÆ
        config_path = os.path.join(self.output_dir, "config.json")
        with open(config_path, 'w') as f:
            json.dump(self.config.__dict__, f, indent=2)
        
        print(f"üìù Êó•Ë™å‰øùÂ≠òÂà∞: {self.output_dir}")
    
    def collect_episode(self):
        """Êî∂ÈõÜ‰∏ÄÂÄãÂÆåÊï¥ÁöÑepisode"""
        # ÈáçÁΩÆÁí∞Â¢ÉÂíåÁ≠ñÁï•ÂåÖË£ùÂô®
        state = self.env.reset()
        self.policy_wrapper.reset_sequence_cache()
        
        # EpisodeÊï∏Êìö
        states = []
        actions = []
        rewards = []
        values = []
        log_probs = []
        dones = []
        
        total_reward = 0
        step_count = 0
        episode_info = {}
        
        while True:
            # Áç≤ÂèñÁ≠ñÁï•Ëº∏Âá∫
            self.policy.eval()
            with torch.no_grad():
                # Áç≤ÂèñÁï∂ÂâçÂ∫èÂàóÊï∏Êìö
                seq_data = self.policy_wrapper.get_sequence_data()
                
                # Áç≤ÂèñÂãï‰ΩúÂíåÂÉπÂÄº
                action, log_prob, entropy, value = self.policy.get_action_and_value(
                    seq_data['states'], 
                    seq_data['actions'], 
                    seq_data['rewards']
                )
                
                action = action.cpu().numpy()
                log_prob = log_prob.cpu().item()
                value = value.cpu().item()
            
            # Âü∑Ë°åÂãï‰Ωú
            next_state, reward, done, info = self.env.step(action)
            
            # Ë®òÈåÑÊï∏Êìö
            states.append(state.copy())
            actions.append(action.copy())
            rewards.append(reward)
            values.append(value)
            log_probs.append(log_prob)
            dones.append(done)
            
            # Êõ¥Êñ∞Á≠ñÁï•ÂåÖË£ùÂô®
            self.policy_wrapper.update_sequence(next_state, action, reward)
            
            # Áµ±Ë®à
            total_reward += reward
            step_count += 1
            self.global_step += 1
            
            # Ê™¢Êü•ÁµÇÊ≠¢Ê¢ù‰ª∂
            if done or step_count >= self.config.max_episode_steps:
                episode_info = {
                    'reason': info.get('reason', 'max_steps'),
                    'final_imu': info.get('imu_data', (0, 0)),
                    'final_gps': info.get('gps_data', (0, 0, 0))
                }
                break
            
            state = next_state
        
        # ËΩâÊèõÁÇ∫ÂºµÈáè
        episode_data = {
            'states': torch.tensor(np.array(states), dtype=torch.float32),
            'actions': torch.tensor(np.array(actions), dtype=torch.float32),
            'rewards': torch.tensor(rewards, dtype=torch.float32),
            'values': torch.tensor(values, dtype=torch.float32),
            'log_probs': torch.tensor(log_probs, dtype=torch.float32),
            'dones': torch.tensor(dones, dtype=torch.float32),
            'total_reward': total_reward,
            'length': step_count,
            'info': episode_info
        }
        
        return episode_data
    
    def collect_experiences(self):
        """Êî∂ÈõÜË®ìÁ∑¥Êï∏Êìö"""
        self.experience_buffer.clear()
        
        for episode_idx in range(self.config.episodes_per_update):
            episode_data = self.collect_episode()
            self.experience_buffer.add_episode(episode_data)
            
            # Á∞°ÂñÆÈÄ≤Â∫¶È°ØÁ§∫
            if episode_idx % max(1, self.config.episodes_per_update // 4) == 0:
                print(f"  Êî∂ÈõÜepisode {episode_idx+1}/{self.config.episodes_per_update}, "
                      f"ÁçéÂãµ: {episode_data['total_reward']:.3f}, "
                      f"Èï∑Â∫¶: {episode_data['length']}")
        
        return self.experience_buffer.get_batch_data()
    
    def compute_advantages_and_returns(self, batch_data):
        """Ë®àÁÆóÊâÄÊúâepisodeÁöÑÂÑ™Âã¢ÂáΩÊï∏ÂíåÂõûÂ†±"""
        all_advantages = []
        all_returns = []
        
        for i in range(len(batch_data['states_seq'])):
            rewards = batch_data['rewards_seq'][i]
            values = batch_data['values_seq'][i]
            dones = batch_data['dones_seq'][i]
            
            # Ë®àÁÆó‰∏ã‰∏ÄÂÄãÁãÄÊÖãÁöÑÂÉπÂÄºÔºàÁî®ÊñºbootstrapÔºâ
            if dones[-1]:
                next_value = 0.0  # EpisodeÁµêÊùüÔºå‰∏ã‰∏ÄÂÄãÁãÄÊÖãÂÉπÂÄºÁÇ∫0
            else:
                # EpisodeÊú™ÁµêÊùüÔºå‰º∞Ë®à‰∏ã‰∏ÄÂÄãÁãÄÊÖãÁöÑÂÉπÂÄº
                # ÈÄôË£°Á∞°ÂåñÁÇ∫‰ΩøÁî®ÊúÄÂæå‰∏ÄÂÄãÁãÄÊÖãÁöÑÂÉπÂÄº
                next_value = values[-1].item()
            
            # Ë®àÁÆóGAE
            advantages, returns = self.gae.compute_advantages(rewards, values, dones, next_value)
            
            all_advantages.append(advantages)
            all_returns.append(returns)
        
        return all_advantages, all_returns
    
    def ppo_update(self, batch_data):
        """Âü∑Ë°åPPOÊõ¥Êñ∞"""
        # Ë®àÁÆóÂÑ™Âã¢ÂáΩÊï∏ÂíåÂõûÂ†±
        all_advantages, all_returns = self.compute_advantages_and_returns(batch_data)
        
        # Ê∫ñÂÇôË®ìÁ∑¥Êï∏Êìö
        train_data = []
        for i in range(len(batch_data['states_seq'])):
            # ÊßãÂª∫Â∫èÂàóÊï∏Êìö
            seq_len = len(batch_data['states_seq'][i])
            
            # ÂâµÂª∫Â°´ÂÖÖÁöÑÂ∫èÂàóÔºàÁ¢∫‰øùÈï∑Â∫¶ÁÇ∫sequence_lengthÔºâ
            padded_states = torch.zeros(self.config.sequence_length, 6)
            padded_actions = torch.zeros(self.config.sequence_length, 6)
            padded_rewards = torch.zeros(self.config.sequence_length)
            
            # Â°´ÂÖÖÂØ¶ÈöõÊï∏Êìö
            actual_len = min(seq_len, self.config.sequence_length)
            padded_states[:actual_len] = batch_data['states_seq'][i][:actual_len]
            padded_actions[:actual_len] = batch_data['actions_seq'][i][:actual_len]
            padded_rewards[:actual_len] = batch_data['rewards_seq'][i][:actual_len]
            
            # ÁÇ∫ÊØèÂÄãÊôÇÈñìÊ≠•ÂâµÂª∫Ë®ìÁ∑¥Êï∏Êìö
            for t in range(actual_len):
                train_data.append({
                    'states_seq': padded_states,
                    'actions_seq': padded_actions,
                    'rewards_seq': padded_rewards,
                    'action': batch_data['actions_seq'][i][t],
                    'old_log_prob': batch_data['log_probs_seq'][i][t],
                    'advantage': all_advantages[i][t],
                    'return': all_returns[i][t],
                    'old_value': batch_data['values_seq'][i][t]
                })
        
        # Ê®ôÊ∫ñÂåñÂÑ™Âã¢ÂáΩÊï∏
        advantages = torch.tensor([data['advantage'] for data in train_data])
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        for i, data in enumerate(train_data):
            data['advantage'] = advantages[i]
        
        # Â§öÊ¨°Êõ¥Êñ∞
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        total_kl_div = 0
        
        for epoch in range(self.config.update_epochs):
            # Èö®Ê©üÊâì‰∫ÇÊï∏Êìö
            np.random.shuffle(train_data)
            
            epoch_policy_loss = 0
            epoch_value_loss = 0
            epoch_entropy_loss = 0
            epoch_kl_div = 0
            
            for data in train_data:
                self.policy.train()
                
                # ÂâçÂêëÂÇ≥Êí≠
                action, log_prob, entropy, value = self.policy.get_action_and_value(
                    data['states_seq'],
                    data['actions_seq'],
                    data['rewards_seq'],
                    action=data['action']
                )
                
                # Ë®àÁÆóÊêçÂ§±
                # 1. Policy Loss (PPO Clipped)
                ratio = torch.exp(log_prob - data['old_log_prob'])
                surr1 = data['advantage'] * ratio
                surr2 = data['advantage'] * torch.clamp(ratio, 1 - self.config.clip_coef, 1 + self.config.clip_coef)
                policy_loss = -torch.min(surr1, surr2)
                
                # 2. Value Loss
                value_loss = 0.5 * (value - data['return']) ** 2
                
                # 3. Entropy Loss
                entropy_loss = -entropy
                
                # 4. Total Loss
                loss = policy_loss + self.config.value_loss_coef * value_loss + self.config.entropy_coef * entropy_loss
                
                # ÂèçÂêëÂÇ≥Êí≠
                self.optimizer.zero_grad()
                loss.backward()
                
                # Ê¢ØÂ∫¶Ë£ÅÂâ™
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)
                
                self.optimizer.step()
                
                # Áµ±Ë®à
                epoch_policy_loss += policy_loss.item()
                epoch_value_loss += value_loss.item()
                epoch_entropy_loss += entropy_loss.item()
                
                # KLÊï£Â∫¶ÔºàÁî®ÊñºÁõ£ÊéßÔºâ
                kl_div = data['old_log_prob'] - log_prob
                epoch_kl_div += kl_div.item()
            
            # EpochÁµ±Ë®à
            num_samples = len(train_data)
            total_policy_loss += epoch_policy_loss / num_samples
            total_value_loss += epoch_value_loss / num_samples
            total_entropy_loss += epoch_entropy_loss / num_samples
            total_kl_div += epoch_kl_div / num_samples
        
        # ËøîÂõûË®ìÁ∑¥Áµ±Ë®à
        num_epochs = self.config.update_epochs
        return {
            'policy_loss': total_policy_loss / num_epochs,
            'value_loss': total_value_loss / num_epochs,
            'entropy_loss': total_entropy_loss / num_epochs,
            'kl_div': total_kl_div / num_epochs,
            'num_samples': len(train_data)
        }
    
    def evaluate_policy(self, num_episodes=5):
        """Ë©ï‰º∞Á≠ñÁï•ÊÄßËÉΩ"""
        print("üß™ Ë©ï‰º∞Á≠ñÁï•ÊÄßËÉΩ...")
        
        eval_rewards = []
        eval_lengths = []
        
        for _ in range(num_episodes):
            episode_data = self.collect_episode()
            eval_rewards.append(episode_data['total_reward'])
            eval_lengths.append(episode_data['length'])
        
        avg_reward = np.mean(eval_rewards)
        avg_length = np.mean(eval_lengths)
        
        print(f"  Ë©ï‰º∞ÁµêÊûú: Âπ≥ÂùáÁçéÂãµ={avg_reward:.3f}, Âπ≥ÂùáÈï∑Â∫¶={avg_length:.1f}")
        
        return {
            'avg_reward': avg_reward,
            'avg_length': avg_length,
            'rewards': eval_rewards,
            'lengths': eval_lengths
        }
    
    def save_model(self, suffix=""):
        """‰øùÂ≠òÊ®°Âûã"""
        save_path = os.path.join(self.output_dir, f"model{suffix}.pt")
        
        save_data = {
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config.__dict__,
            'global_step': self.global_step,
            'update_count': self.update_count,
            'best_avg_reward': self.best_avg_reward
        }
        
        torch.save(save_data, save_path)
        print(f"üíæ Ê®°ÂûãÂ∑≤‰øùÂ≠ò: {save_path}")
        
        return save_path
    
    def load_model(self, checkpoint_path):
        """ËºâÂÖ•Ê®°Âûã"""
        checkpoint = torch.load(checkpoint_path)
        
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.global_step = checkpoint['global_step']
        self.update_count = checkpoint['update_count']
        self.best_avg_reward = checkpoint['best_avg_reward']
        
        print(f"üìÅ Ê®°ÂûãÂ∑≤ËºâÂÖ•: {checkpoint_path}")
    
    def update_learning_rate(self):
        """Êõ¥Êñ∞Â≠∏ÁøíÁéá"""
        if self.config.anneal_lr:
            frac = 1.0 - self.global_step / self.config.total_timesteps
            new_lr = frac * self.config.learning_rate
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
    
    def log_metrics(self, metrics):
        """Ë®òÈåÑË®ìÁ∑¥ÊåáÊ®ô"""
        # TensorBoard
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, self.update_count)
        
        # Weights & Biases
        if self.use_wandb:
            wandb.log(metrics, step=self.update_count)
    
    def train(self):
        """‰∏ªË®ìÁ∑¥Âæ™Áí∞"""
        print("üöÄ ÈñãÂßãË®ìÁ∑¥...")
        print(f"ÁõÆÊ®ô: {self.config.total_timesteps:,} Á∏ΩÊ≠•Êï∏")
        
        start_time = time.time()
        
        while self.global_step < self.config.total_timesteps:
            self.update_count += 1
            
            # Êî∂ÈõÜÁ∂ìÈ©ó
            print(f"\nüìä Êõ¥Êñ∞ {self.update_count} - Êî∂ÈõÜÁ∂ìÈ©ó...")
            batch_data = self.collect_experiences()
            
            # PPOÊõ¥Êñ∞
            print("üîÑ Âü∑Ë°åPPOÊõ¥Êñ∞...")
            update_stats = self.ppo_update(batch_data)
            
            # Êõ¥Êñ∞Â≠∏ÁøíÁéá
            self.update_learning_rate()
            
            # Ë®àÁÆóÁµ±Ë®à‰ø°ÊÅØ
            avg_reward = np.mean(batch_data['episode_rewards'])
            avg_length = np.mean(batch_data['episode_lengths'])
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # Êó•Ë™åË®òÈåÑ
            if self.update_count % self.config.log_frequency == 0:
                elapsed_time = time.time() - start_time
                steps_per_sec = self.global_step / elapsed_time
                
                metrics = {
                    'train/avg_reward': avg_reward,
                    'train/avg_length': avg_length,
                    'train/policy_loss': update_stats['policy_loss'],
                    'train/value_loss': update_stats['value_loss'],
                    'train/entropy_loss': update_stats['entropy_loss'],
                    'train/kl_div': update_stats['kl_div'],
                    'train/learning_rate': current_lr,
                    'train/global_step': self.global_step,
                    'train/steps_per_sec': steps_per_sec
                }
                
                self.log_metrics(metrics)
                
                print(f"üìà Êõ¥Êñ∞ {self.update_count}: ÁçéÂãµ={avg_reward:.3f}, "
                      f"Èï∑Â∫¶={avg_length:.1f}, SPS={steps_per_sec:.1f}")
            
            # ÂÆöÊúüË©ï‰º∞
            if self.update_count % self.config.eval_frequency == 0:
                eval_stats = self.evaluate_policy()
                
                eval_metrics = {
                    'eval/avg_reward': eval_stats['avg_reward'],
                    'eval/avg_length': eval_stats['avg_length']
                }
                self.log_metrics(eval_metrics)
                
                # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
                if eval_stats['avg_reward'] > self.best_avg_reward:
                    self.best_avg_reward = eval_stats['avg_reward']
                    self.save_model("_best")
            
            # ÂÆöÊúü‰øùÂ≠ò
            if self.update_count % self.config.save_frequency == 0:
                self.save_model(f"_update_{self.update_count}")
            
            # Êó©ÂÅúÊ™¢Êü•
            if avg_reward >= self.config.target_reward:
                print(f"üéØ ÈÅîÂà∞ÁõÆÊ®ôÁçéÂãµ {self.config.target_reward}! Ë®ìÁ∑¥ÂÆåÊàê!")
                break
        
        # Ë®ìÁ∑¥ÂÆåÊàê
        print("\n‚úÖ Ë®ìÁ∑¥ÂÆåÊàê!")
        self.save_model("_final")
        
        # ÊúÄÁµÇË©ï‰º∞
        final_eval = self.evaluate_policy(num_episodes=10)
        print(f"üèÜ ÊúÄÁµÇË©ï‰º∞: Âπ≥ÂùáÁçéÂãµ={final_eval['avg_reward']:.3f}")
        
        # Ê∏ÖÁêÜ
        self.writer.close()
        if self.use_wandb:
            wandb.finish()


def main():
    """‰∏ªÂáΩÊï∏"""
    print("ü§ñ ÂÖ≠Ë∂≥Ê©üÂô®‰∫∫Âπ≥Ë°°Ë®ìÁ∑¥ - PPO + Transformer")
    
    # ÈÖçÁΩÆ
    config = PPOConfig(
        # Âü∫Êú¨ÈÖçÁΩÆ
        total_timesteps=500000,
        episodes_per_update=4,
        update_epochs=4,
        
        # Á∂≤Ë∑ØÈÖçÁΩÆ
        hidden_size=128,
        n_layer=3,
        n_head=2,
        
        # PPOÈÖçÁΩÆ
        learning_rate=3e-4,
        clip_coef=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        
        # Êó•Ë™åÈÖçÁΩÆ
        use_wandb=False,  # Ë®≠ÁÇ∫TrueÂïüÁî®Weights & Biases
        run_name=f"hexapod_ppo_{int(time.time())}"
    )
    
    # ÂâµÂª∫Áí∞Â¢É
    print("üåç ÂàùÂßãÂåñÁí∞Â¢É...")
    from hexapod_balance_env import HexapodBalanceEnv
    env = HexapodBalanceEnv(
        max_episode_steps=config.max_episode_steps,
        sequence_length=config.sequence_length
    )
    
    # ÂâµÂª∫Á≠ñÁï•Á∂≤Ë∑Ø
    print("üß† ÂàùÂßãÂåñTransformerÁ≠ñÁï•Á∂≤Ë∑Ø...")
    from transformer_policy import TransformerPolicyNetwork
    policy = TransformerPolicyNetwork(
        state_dim=6,
        action_dim=6,
        sequence_length=config.sequence_length,
        hidden_size=config.hidden_size,
        n_layer=config.n_layer,
        n_head=config.n_head,
        dropout=config.dropout,
        action_range=config.action_range
    )
    
    # ÂâµÂª∫Ë®ìÁ∑¥Âô®
    trainer = PPOTrainer(env, policy, config)
    
    # ÈñãÂßãË®ìÁ∑¥
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Ë®ìÁ∑¥Ë¢´‰∏≠Êñ∑")
        trainer.save_model("_interrupted")
    except Exception as e:
        print(f"\n‚ùå Ë®ìÁ∑¥Âá∫ÈåØ: {e}")
        import traceback
        traceback.print_exc()
        trainer.save_model("_error")
    finally:
        print("üßπ Ê∏ÖÁêÜË≥áÊ∫ê...")
        env.close()


if __name__ == "__main__":
    main()