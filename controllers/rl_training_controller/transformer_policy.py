import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.distributions import Normal
import math


class PositionalEncoding(nn.Module):
    """ä½ç½®ç·¨ç¢¼æ¨¡çµ„"""
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: [seq_len, batch_size, d_model]
        """
        return x + self.pe[:x.size(0), :]


class TransformerBlock(nn.Module):
    """å–®å€‹Transformerå±¤"""
    
    def __init__(self, d_model, n_head, dropout=0.1):
        super().__init__()
        
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_head,
            dropout=dropout,
            batch_first=False  # ä½¿ç”¨ [seq_len, batch, features] æ ¼å¼
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        """
        Args:
            x: [seq_len, batch_size, d_model]
            mask: [seq_len, seq_len] æ³¨æ„åŠ›é®ç½©
        """
        # Self-attention with residual connection
        attn_output, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        
        return x


class SequenceEmbedding(nn.Module):
    """åºåˆ—åµŒå…¥æ¨¡çµ„ï¼šå°‡ç‹€æ…‹-å‹•ä½œ-çå‹µåºåˆ—è½‰æ›ç‚ºçµ±ä¸€çš„åµŒå…¥"""
    
    def __init__(self, state_dim, action_dim, hidden_size):
        super().__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        
        # åˆ†åˆ¥çš„åµŒå…¥å±¤
        self.state_embed = nn.Linear(state_dim, hidden_size)
        self.action_embed = nn.Linear(action_dim, hidden_size)
        self.reward_embed = nn.Linear(1, hidden_size)  # çå‹µæ˜¯æ¨™é‡
        
        # é¡å‹åµŒå…¥ï¼ˆå€åˆ†ç‹€æ…‹ã€å‹•ä½œã€çå‹µï¼‰
        self.type_embed = nn.Embedding(3, hidden_size)  # 0:state, 1:action, 2:reward
        
        # æ™‚é–“æ­¥åµŒå…¥
        self.timestep_embed = nn.Embedding(5000, hidden_size)  # æ”¯æ´æœ€å¤š1000æ­¥
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, states, actions, rewards, timesteps=None):
        """
        Args:
            states: [seq_len, state_dim] æˆ– [batch_size, seq_len, state_dim]
            actions: [seq_len, action_dim] æˆ– [batch_size, seq_len, action_dim]
            rewards: [seq_len] æˆ– [batch_size, seq_len]
            timesteps: [seq_len] æˆ– [batch_size, seq_len] å¯é¸çš„æ™‚é–“æ­¥
        
        Returns:
            embedded_sequence: [seq_len * 3, batch_size, hidden_size]
                æŒ‰ç…§ [s_0, a_0, r_0, s_1, a_1, r_1, ...] çš„é †åºæ’åˆ—
        """
        # è‡ªå‹•è™•ç†å–®æ¨£æœ¬å’Œæ‰¹æ¬¡è¼¸å…¥
        if len(states.shape) == 2:  # å–®æ¨£æœ¬ [seq_len, state_dim]
            states = states.unsqueeze(0)    # [1, seq_len, state_dim]
            actions = actions.unsqueeze(0)  # [1, seq_len, action_dim]
            rewards = rewards.unsqueeze(0)  # [1, seq_len]
            single_sample = True
        else:
            single_sample = False
            
        batch_size, seq_len = states.shape[:2]
        
        # åµŒå…¥å„å€‹æ¨¡æ…‹
        state_embeds = self.state_embed(states)     # [batch, seq_len, hidden]
        action_embeds = self.action_embed(actions)  # [batch, seq_len, hidden]
        reward_embeds = self.reward_embed(rewards.unsqueeze(-1))  # [batch, seq_len, hidden]
        
        # åŠ å…¥é¡å‹åµŒå…¥
        type_ids = torch.tensor([0, 1, 2], device=states.device)  # state, action, reward
        type_embeds = self.type_embed(type_ids)  # [3, hidden]
        
        state_embeds = state_embeds + type_embeds[0]
        action_embeds = action_embeds + type_embeds[1]
        reward_embeds = reward_embeds + type_embeds[2]
        
        # åŠ å…¥æ™‚é–“æ­¥åµŒå…¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if timesteps is not None:
            time_embeds = self.timestep_embed(timesteps)  # [batch, seq_len, hidden]
            state_embeds = state_embeds + time_embeds
            action_embeds = action_embeds + time_embeds
            reward_embeds = reward_embeds + time_embeds
        
        # äº¤éŒ¯æ’åˆ—ï¼š[s_0, a_0, r_0, s_1, a_1, r_1, ...]
        # è½‰æ›ç‚º [seq_len, batch, hidden] æ ¼å¼
        state_embeds = state_embeds.transpose(0, 1)   # [seq_len, batch, hidden]
        action_embeds = action_embeds.transpose(0, 1) # [seq_len, batch, hidden]
        reward_embeds = reward_embeds.transpose(0, 1) # [seq_len, batch, hidden]
        
        # äº¤éŒ¯åˆä½µ
        sequence_embeds = []
        for t in range(seq_len):
            sequence_embeds.append(state_embeds[t])   # s_t
            sequence_embeds.append(action_embeds[t])  # a_t
            sequence_embeds.append(reward_embeds[t])  # r_t
        
        # å †ç–Šç‚º [seq_len * 3, batch, hidden]
        embedded_sequence = torch.stack(sequence_embeds, dim=0)
        
        return self.dropout(embedded_sequence)


class TransformerPolicyNetwork(nn.Module):
    """
    Transformer Policy Network for PPO
    åƒè€ƒDecision Transformerçš„åºåˆ—å»ºæ¨¡æ–¹å¼
    """
    
    def __init__(
        self,
        state_dim=6,           # ç‹€æ…‹ç¶­åº¦
        action_dim=6,          # å‹•ä½œç¶­åº¦
        sequence_length=50,    # åºåˆ—é•·åº¦
        hidden_size=128,       # éš±è—å±¤ç¶­åº¦
        n_layer=3,             # Transformerå±¤æ•¸
        n_head=2,              # æ³¨æ„åŠ›é ­æ•¸
        dropout=0.1,           # Dropoutç‡
        action_range=1.0       # å‹•ä½œç¯„åœ [-action_range, action_range]
    ):
        super().__init__()

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ® ä½¿ç”¨è¨­å‚™: {self.device}")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        self.n_layer = n_layer
        self.n_head = n_head
        self.action_range = action_range
        
        # åºåˆ—åµŒå…¥æ¨¡çµ„
        self.embedding = SequenceEmbedding(state_dim, action_dim, hidden_size)
        
        # ä½ç½®ç·¨ç¢¼
        self.pos_encoding = PositionalEncoding(hidden_size, max_len=sequence_length * 3)
        
        # Transformerå±¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(hidden_size, n_head, dropout)
            for _ in range(n_layer)
        ])
        
        # å±¤æ¨™æº–åŒ–
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        # Policyé ­ï¼ˆè¼¸å‡ºå‹•ä½œçš„å‡å€¼å’Œæ¨™æº–å·®ï¼‰
        self.policy_mean = nn.Linear(hidden_size, action_dim)
        self.policy_logstd = nn.Linear(hidden_size, action_dim)
        
        # Valueé ­ï¼ˆç‹€æ…‹åƒ¹å€¼ä¼°è¨ˆï¼‰
        self.value_head = nn.Linear(hidden_size, 1)
        
        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)
        self.to(self.device)
        
        print(f"âœ… Transformer Policy Network åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š åƒæ•¸: state_dim={state_dim}, action_dim={action_dim}")
        print(f"ğŸ”§ æ¶æ§‹: seq_len={sequence_length}, hidden={hidden_size}, layers={n_layer}, heads={n_head}")
        print(f"ğŸ“ˆ ç¸½åƒæ•¸é‡: {self.count_parameters():,}")
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¬Šé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def count_parameters(self):
        """è¨ˆç®—æ¨¡å‹åƒæ•¸é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def _ensure_device(self, tensor):
        """ç¢ºä¿å¼µé‡åœ¨æ­£ç¢ºè¨­å‚™ä¸Š"""
        if isinstance(tensor, np.ndarray):
            return torch.tensor(tensor, dtype=torch.float32).to(self.device)
        elif isinstance(tensor, torch.Tensor):
            return tensor.to(self.device)
        else:
            return torch.tensor(tensor, dtype=torch.float32).to(self.device)
    
    def create_causal_mask(self, seq_len):
        """å‰µå»ºå› æœé®ç½©ï¼Œç¢ºä¿ç•¶å‰ä½ç½®åªèƒ½çœ‹åˆ°éå»çš„ä¿¡æ¯"""
        mask = torch.tril(torch.ones(seq_len, seq_len, device=self.device)).bool()
        return ~mask
    
    def forward(self, states, actions, rewards, return_dict=False):
        """
        å‰å‘å‚³æ’­
        
        Args:
            states: [seq_len, state_dim] æˆ– [batch_size, seq_len, state_dim]
            actions: [seq_len, action_dim] æˆ– [batch_size, seq_len, action_dim]
            rewards: [seq_len] æˆ– [batch_size, seq_len]
            return_dict: æ˜¯å¦è¿”å›è©³ç´°å­—å…¸
        
        Returns:
            å¦‚æœreturn_dict=False: (action_mean, action_logstd, value)
            å¦‚æœreturn_dict=True: åŒ…å«è©³ç´°ä¿¡æ¯çš„å­—å…¸
        """
        states = self._ensure_device(states)
        actions = self._ensure_device(actions)
        rewards = self._ensure_device(rewards)
        # è‡ªå‹•è™•ç†å–®æ¨£æœ¬è¼¸å…¥
        if len(states.shape) == 2:  # å–®æ¨£æœ¬ [seq_len, state_dim]
            states = states.unsqueeze(0)    # [1, seq_len, state_dim]
            actions = actions.unsqueeze(0)  # [1, seq_len, action_dim]
            rewards = rewards.unsqueeze(0)  # [1, seq_len]
            single_sample = True
        else:
            single_sample = False
            
        batch_size, seq_len = states.shape[:2]
        
        # åºåˆ—åµŒå…¥
        embedded_sequence = self.embedding(states, actions, rewards)
        # embedded_sequence: [seq_len * 3, batch_size, hidden_size]
        
        # ä½ç½®ç·¨ç¢¼
        embedded_sequence = self.pos_encoding(embedded_sequence)
        
        # å‰µå»ºå› æœé®ç½©
        total_seq_len = embedded_sequence.shape[0]
        causal_mask = self.create_causal_mask(total_seq_len).to(embedded_sequence.device)
        
        # é€šéTransformerå±¤
        hidden_states = embedded_sequence
        for transformer_block in self.transformer_blocks:
            hidden_states = transformer_block(hidden_states, mask=causal_mask)
        
        # å±¤æ¨™æº–åŒ–
        hidden_states = self.layer_norm(hidden_states)
        
        # æå–ç‹€æ…‹å°æ‡‰çš„éš±è—ç‹€æ…‹ï¼ˆåºåˆ—ä¸­çš„ç‹€æ…‹ä½ç½®ï¼š0, 3, 6, 9, ...ï¼‰
        state_indices = torch.arange(0, total_seq_len, 3)  # [0, 3, 6, 9, ...]
        state_hidden = hidden_states[state_indices]  # [seq_len, batch_size, hidden_size]
        
        # è½‰æ›å› [batch_size, seq_len, hidden_size]
        state_hidden = state_hidden.transpose(0, 1)
        
        # åªä½¿ç”¨æœ€å¾Œä¸€å€‹ç‹€æ…‹çš„éš±è—ç‹€æ…‹ä¾†é æ¸¬å‹•ä½œå’Œåƒ¹å€¼
        last_hidden = state_hidden[:, -1, :]  # [batch_size, hidden_size]
        
        # Policyè¼¸å‡ºï¼ˆå‹•ä½œçš„å‡å€¼å’Œå°æ•¸æ¨™æº–å·®ï¼‰
        action_mean = self.policy_mean(last_hidden)
        action_logstd = self.policy_logstd(last_hidden)
        
        # é™åˆ¶å‹•ä½œç¯„åœ
        action_mean = torch.tanh(action_mean) * self.action_range
        action_logstd = torch.clamp(action_logstd, min=-20, max=2)
        
        # Valueè¼¸å‡º
        value = self.value_head(last_hidden).squeeze(-1)  # [batch_size] æˆ– [1]
        
        # å¦‚æœæ˜¯å–®æ¨£æœ¬è¼¸å…¥ï¼Œç§»é™¤æ‰¹æ¬¡ç¶­åº¦
        if single_sample:
            action_mean = action_mean.squeeze(0)    # [action_dim]
            action_logstd = action_logstd.squeeze(0) # [action_dim]
            value = value.squeeze(0)                # scalar
        
        if return_dict:
            return {
                'action_mean': action_mean,
                'action_logstd': action_logstd,
                'action_std': torch.exp(action_logstd),
                'value': value,
                'hidden_states': hidden_states,
                'state_hidden': state_hidden,
                'last_hidden': last_hidden
            }
        else:
            return action_mean, action_logstd, value
    
    def get_action_and_value(self, states, actions, rewards, action=None):
        """
        ç²å–å‹•ä½œå’Œåƒ¹å€¼ï¼ˆç”¨æ–¼PPOè¨“ç·´ï¼‰
        
        Args:
            states: [seq_len, state_dim] æˆ– [batch_size, seq_len, state_dim]
            actions: [seq_len, action_dim] æˆ– [batch_size, seq_len, action_dim]
            rewards: [seq_len] æˆ– [batch_size, seq_len]
            action: [action_dim] æˆ– [batch_size, action_dim] å¯é¸ï¼Œç”¨æ–¼è¨ˆç®—ç‰¹å®šå‹•ä½œçš„æ¦‚ç‡
        
        Returns:
            sampled_action: [action_dim] æˆ– [batch_size, action_dim] æ¡æ¨£çš„å‹•ä½œ
            log_prob: scalar æˆ– [batch_size] å‹•ä½œçš„å°æ•¸æ¦‚ç‡
            entropy: scalar æˆ– [batch_size] ç­–ç•¥ç†µ
            value: scalar æˆ– [batch_size] ç‹€æ…‹åƒ¹å€¼
        """
        states = self._ensure_device(states)
        actions = self._ensure_device(actions)
        rewards = self._ensure_device(rewards)
        if action is not None:
            action = self._ensure_device(action)
        action_mean, action_logstd, value = self.forward(states, actions, rewards)
        action_std = torch.exp(action_logstd)
        
        # å‰µå»ºæ­£æ…‹åˆ†ä½ˆ
        dist = Normal(action_mean, action_std)
        
        # å¦‚æœæ²’æœ‰æä¾›ç‰¹å®šå‹•ä½œï¼Œå‰‡æ¡æ¨£
        if action is None:
            sampled_action = dist.sample()
        else:
            sampled_action = action
        
        # è¨ˆç®—å°æ•¸æ¦‚ç‡
        log_prob = dist.log_prob(sampled_action).sum(dim=-1)  # å°å‹•ä½œç¶­åº¦æ±‚å’Œ
        
        # è¨ˆç®—ç†µ
        entropy = dist.entropy().sum(dim=-1)
        
        # é™åˆ¶å‹•ä½œç¯„åœ
        sampled_action = torch.clamp(sampled_action, -self.action_range, self.action_range)
        
        return sampled_action, log_prob, entropy, value
    
    def get_value(self, states, actions, rewards):
        """
        åƒ…ç²å–ç‹€æ…‹åƒ¹å€¼ï¼ˆç”¨æ–¼å„ªå‹¢å‡½æ•¸è¨ˆç®—ï¼‰
        
        Args:
            states: [seq_len, state_dim] æˆ– [batch_size, seq_len, state_dim]
            actions: [seq_len, action_dim] æˆ– [batch_size, seq_len, action_dim]
            rewards: [seq_len] æˆ– [batch_size, seq_len]
        
        Returns:
            value: scalar æˆ– [batch_size] ç‹€æ…‹åƒ¹å€¼
        """
        states = self._ensure_device(states)
        actions = self._ensure_device(actions)
        rewards = self._ensure_device(rewards)
        _, _, value = self.forward(states, actions, rewards)
        return value
    
    def get_action_distribution(self, states, actions, rewards):
        """
        ç²å–å‹•ä½œåˆ†ä½ˆï¼ˆç”¨æ–¼ç­–ç•¥è©•ä¼°ï¼‰
        
        Args:
            states: [seq_len, state_dim] æˆ– [batch_size, seq_len, state_dim]
            actions: [seq_len, action_dim] æˆ– [batch_size, seq_len, action_dim]
            rewards: [seq_len] æˆ– [batch_size, seq_len]
        
        Returns:
            dist: torch.distributions.Normal å‹•ä½œåˆ†ä½ˆ
        """
        states = self._ensure_device(states)
        actions = self._ensure_device(actions)
        rewards = self._ensure_device(rewards)
        action_mean, action_logstd, _ = self.forward(states, actions, rewards)
        action_std = torch.exp(action_logstd)
        return Normal(action_mean, action_std)


class TransformerPolicyWrapper:
    """
    Transformer Policyçš„åŒ…è£å™¨ï¼Œæä¾›ä¾¿åˆ©çš„ä»‹é¢ç”¨æ–¼èˆ‡ç’°å¢ƒäº’å‹•
    """
    
    def __init__(self, policy_network, device=None):
        if device is None:
            self.device = policy_network.device
        else:
            self.device = device
        self.policy = policy_network.to(device)
        
        # ç”¨æ–¼æ¨ç†æ™‚çš„åºåˆ—ç·©å­˜
        self.reset_sequence_cache()
    
    def reset_sequence_cache(self):
        """é‡ç½®åºåˆ—ç·©å­˜"""
        seq_len = self.policy.sequence_length
        state_dim = self.policy.state_dim
        action_dim = self.policy.action_dim
        
        self.states_cache = torch.zeros(1, seq_len, state_dim).to(self.device)
        self.actions_cache = torch.zeros(1, seq_len, action_dim).to(self.device)
        self.rewards_cache = torch.zeros(1, seq_len).to(self.device)
        self.cache_index = 0
    
    def update_sequence(self, state, action=None, reward=0.0):
        """æ›´æ–°åºåˆ—ç·©å­˜"""
        if self.cache_index < self.policy.sequence_length:
            # å¡«å……éšæ®µ
            self.states_cache[0, self.cache_index] = torch.tensor(state).to(self.device)
            if action is not None:
                self.actions_cache[0, self.cache_index] = torch.tensor(action).to(self.device)
            self.rewards_cache[0, self.cache_index] = reward
            self.cache_index += 1
        else:
            # æ»‘å‹•çª—å£éšæ®µ
            self.states_cache = torch.roll(self.states_cache, -1, dims=1)
            self.actions_cache = torch.roll(self.actions_cache, -1, dims=1)
            self.rewards_cache = torch.roll(self.rewards_cache, -1, dims=1)
            
            self.states_cache[0, -1] = torch.tensor(state).to(self.device)
            if action is not None:
                self.actions_cache[0, -1] = torch.tensor(action).to(self.device)
            self.rewards_cache[0, -1] = reward
    
    def predict_action(self, state, deterministic=False):
        """
        é æ¸¬å‹•ä½œï¼ˆç”¨æ–¼ç’°å¢ƒäº’å‹•ï¼‰
        
        Args:
            state: [state_dim] ç•¶å‰ç‹€æ…‹
            deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§å‹•ä½œ
        
        Returns:
            action: [action_dim] å‹•ä½œ
        """
        # æ›´æ–°ç‹€æ…‹åˆ°åºåˆ—ä¸­
        self.update_sequence(state)
        
        self.policy.eval()
        with torch.no_grad():
            if deterministic:
                action_mean, _, _ = self.policy(
                    self.states_cache.squeeze(0),   # [seq_len, state_dim]
                    self.actions_cache.squeeze(0),  # [seq_len, action_dim]
                    self.rewards_cache.squeeze(0)   # [seq_len]
                )
                action = action_mean.cpu().numpy()  # [action_dim]
            else:
                sampled_action, _, _, _ = self.policy.get_action_and_value(
                    self.states_cache.squeeze(0),   # [seq_len, state_dim]
                    self.actions_cache.squeeze(0),  # [seq_len, action_dim]
                    self.rewards_cache.squeeze(0)   # [seq_len]
                )
                action = sampled_action.cpu().numpy()  # [action_dim]
        
        # æ›´æ–°å‹•ä½œåˆ°åºåˆ—ä¸­
        self.update_sequence(state, action)
        
        return action
    
    def get_value(self, state):
        """ç²å–ç‹€æ…‹åƒ¹å€¼"""
        self.policy.eval()
        with torch.no_grad():
            value = self.policy.get_value(
                self.states_cache.squeeze(0),   # [seq_len, state_dim]
                self.actions_cache.squeeze(0),  # [seq_len, action_dim]
                self.rewards_cache.squeeze(0)   # [seq_len]
            )
        return value.cpu().item()  # è¿”å›æ¨™é‡
    
    def get_sequence_data(self):
        """ç²å–ç•¶å‰åºåˆ—æ•¸æ“šï¼ˆä¾›è¨“ç·´å™¨ä½¿ç”¨ï¼‰"""
        return {
            'states': self.states_cache.squeeze(0),   # [seq_len, state_dim]
            'actions': self.actions_cache.squeeze(0), # [seq_len, action_dim]  
            'rewards': self.rewards_cache.squeeze(0)  # [seq_len]
        }


# æ¸¬è©¦å’Œä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # å‰µå»ºæ¸¬è©¦æ•¸æ“šï¼ˆå–®æ¨£æœ¬ï¼‰
    sequence_length = 50
    state_dim = 6
    action_dim = 6
    
    # å–®æ¨£æœ¬æ¸¬è©¦æ•¸æ“š
    states = torch.randn(sequence_length, state_dim)   # [50, 6]
    actions = torch.randn(sequence_length, action_dim) # [50, 6]
    rewards = torch.randn(sequence_length)             # [50]
    
    print("ğŸ§ª æ¸¬è©¦ Transformer Policy Network (å–®æ¨£æœ¬æ¨¡å¼)...")
    
    # å‰µå»ºæ¨¡å‹
    policy = TransformerPolicyNetwork(
        state_dim=state_dim,
        action_dim=action_dim,
        sequence_length=sequence_length,
        hidden_size=128,
        n_layer=3,
        n_head=2,
        dropout=0.1,
        action_range=1.0
    )
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    print("\nğŸ“ˆ æ¸¬è©¦å‰å‘å‚³æ’­...")
    action_mean, action_logstd, value = policy(states, actions, rewards)
    print(f"Action mean shape: {action_mean.shape}")     # [6]
    print(f"Action logstd shape: {action_logstd.shape}") # [6]
    print(f"Value shape: {value.shape}")                 # scalar
    
    # æ¸¬è©¦å‹•ä½œæ¡æ¨£
    print("\nğŸ¯ æ¸¬è©¦å‹•ä½œæ¡æ¨£...")
    sampled_action, log_prob, entropy, value = policy.get_action_and_value(states, actions, rewards)
    print(f"Sampled action shape: {sampled_action.shape}") # [6]
    print(f"Log prob shape: {log_prob.shape}")             # scalar
    print(f"Entropy shape: {entropy.shape}")               # scalar
    print(f"Value shape: {value.shape}")                   # scalar
    
    # æ¸¬è©¦åŒ…è£å™¨
    print("\nğŸ”§ æ¸¬è©¦ç­–ç•¥åŒ…è£å™¨...")
    wrapper = TransformerPolicyWrapper(policy)
    
    test_state = np.random.randn(state_dim)
    action = wrapper.predict_action(test_state)
    value = wrapper.get_value(test_state)
    
    print(f"Predicted action: {action}")
    print(f"State value: {value}")
    
    print("\nâœ… Transformer Policy Network æ¸¬è©¦å®Œæˆï¼")