"""
å…­è¶³æ©Ÿå™¨äººRLè¨“ç·´ä¸»æ§åˆ¶å™¨
æ•´åˆWebotsç’°å¢ƒèˆ‡PPO+Transformerè¨“ç·´ç³»çµ±

é€™æ˜¯Webotsæ§åˆ¶å™¨çš„ä¸»å…¥å£æª”æ¡ˆï¼Œè² è²¬ï¼š
1. åˆå§‹åŒ–æ‰€æœ‰ç³»çµ±çµ„ä»¶
2. é¸æ“‡é‹è¡Œæ¨¡å¼ï¼ˆè¨“ç·´/æ¸¬è©¦/çºŒè¨“ï¼‰
3. å”èª¿ç’°å¢ƒã€ç­–ç•¥ç¶²è·¯å’Œè¨“ç·´å™¨
"""

import sys
import os
import time

# ç¢ºä¿å¯ä»¥å°å…¥åŒç›®éŒ„ä¸‹çš„å…¶ä»–æ¨¡çµ„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def print_banner():
    """é¡¯ç¤ºç³»çµ±æ¨™é¡Œ"""
    print("=" * 60)
    print("ğŸ¤– å…­è¶³æ©Ÿå™¨äººå¼·åŒ–å­¸ç¿’è¨“ç·´ç³»çµ±")
    print("ğŸ§  Transformer + PPO + Webots")
    print("=" * 60)

def get_run_mode():
    """
    ç¢ºå®šé‹è¡Œæ¨¡å¼
    åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œå¯ä»¥é€šéç’°å¢ƒè®Šé‡æˆ–æª”æ¡ˆé…ç½®ä¾†æ§åˆ¶
    """
    # æª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å¼é…ç½®æª”æ¡ˆ
    mode_file = os.path.join(current_dir, "run_mode.txt")
    
    if os.path.exists(mode_file):
        with open(mode_file, 'r') as f:
            mode = f.read().strip().lower()
        print(f"ğŸ“ å¾æª”æ¡ˆè®€å–æ¨¡å¼: {mode}")
    else:
        # é è¨­ç‚ºè¨“ç·´æ¨¡å¼
        mode = "train"
        print(f"ğŸ”§ ä½¿ç”¨é è¨­æ¨¡å¼: {mode}")
    
    return mode

def train_new_model():
    """è¨“ç·´æ–°æ¨¡å‹"""
    print("\nğŸš€ é–‹å§‹è¨“ç·´æ–°æ¨¡å‹...")
    
    try:
        # å°å…¥è¨“ç·´é…ç½®
        from ppo_training_system import PPOConfig, PPOTrainer, main as train_main
        from hexapod_balance_env import HexapodBalanceEnv
        from transformer_policy import TransformerPolicyNetwork
        
        print("âœ… æ‰€æœ‰æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
        
        # ä½¿ç”¨é è¨­é…ç½®æˆ–è‡ªå®šç¾©é…ç½®
        config = get_training_config()
        print(f"âš™ï¸  è¨“ç·´é…ç½®: {config.total_timesteps:,} æ­¥, LR={config.learning_rate}")
        
        # å•Ÿå‹•è¨“ç·´ä¸»ç¨‹åº
        train_main()
        
    except ImportError as e:
        print(f"âŒ æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿æ‰€æœ‰Pythonæª”æ¡ˆéƒ½åœ¨æ­£ç¢ºçš„ç›®éŒ„ä¸­")
        return False
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def test_trained_model():
    """æ¸¬è©¦å·²è¨“ç·´çš„æ¨¡å‹"""
    print("\nğŸ§ª æ¸¬è©¦å·²è¨“ç·´æ¨¡å‹...")
    
    try:
        # å°å…¥å¿…è¦æ¨¡çµ„
        from hexapod_balance_env import HexapodBalanceEnv
        from transformer_policy import TransformerPolicyNetwork, TransformerPolicyWrapper
        import torch
        
        # å°‹æ‰¾æœ€ä½³æ¨¡å‹
        model_path = find_best_model()
        if not model_path:
            print("âŒ æ‰¾ä¸åˆ°å·²è¨“ç·´çš„æ¨¡å‹ï¼Œè«‹å…ˆé€²è¡Œè¨“ç·´")
            return False
        
        # âœ… æ™ºèƒ½è¨­å‚™é¸æ“‡
        print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {model_path}")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ® ä½¿ç”¨è¨­å‚™: {device}")
        # å‰µå»ºç’°å¢ƒ
        env = HexapodBalanceEnv(max_episode_steps=2000, sequence_length=50)
        print("âœ… ç’°å¢ƒå‰µå»ºå®Œæˆ")
        
        # å‰µå»ºç­–ç•¥ç¶²è·¯
        policy = TransformerPolicyNetwork(
            state_dim=6,
            action_dim=6,
            sequence_length=50,
            hidden_size=128,
            n_layer=3,
            n_head=2
        )
        print("âœ… ç­–ç•¥ç¶²è·¯å‰µå»ºå®Œæˆ")
        
        # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
        checkpoint = torch.load(model_path, map_location=device)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        print("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥å®Œæˆ")
        
        # å‰µå»ºç­–ç•¥åŒ…è£å™¨
        wrapper = TransformerPolicyWrapper(policy)
        
        # é‹è¡Œæ¸¬è©¦episodes
        run_test_episodes(env, wrapper, num_episodes=5)
        
        env.close()
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def resume_training():
    """çºŒè¨“ç¾æœ‰æ¨¡å‹"""
    print("\nğŸ”„ çºŒè¨“ç¾æœ‰æ¨¡å‹...")
    
    try:
        from ppo_training_system import PPOConfig, PPOTrainer
        from hexapod_balance_env import HexapodBalanceEnv
        from transformer_policy import TransformerPolicyNetwork
        
        # å°‹æ‰¾æœ€æ–°çš„æª¢æŸ¥é»
        checkpoint_path = find_latest_checkpoint()
        if not checkpoint_path:
            print("âŒ æ‰¾ä¸åˆ°æª¢æŸ¥é»ï¼Œè«‹å…ˆé€²è¡Œè¨“ç·´æˆ–æª¢æŸ¥æª”æ¡ˆè·¯å¾‘")
            return False
        
        print(f"ğŸ“ è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")
        
        # å‰µå»ºç³»çµ±çµ„ä»¶
        config = get_training_config()
        env = HexapodBalanceEnv(
            max_episode_steps=config.max_episode_steps,
            sequence_length=config.sequence_length
        )
        policy = TransformerPolicyNetwork(
            state_dim=6,
            action_dim=6,
            sequence_length=config.sequence_length,
            hidden_size=config.hidden_size,
            n_layer=config.n_layer,
            n_head=config.n_head
        )
        
        # å‰µå»ºè¨“ç·´å™¨ä¸¦è¼‰å…¥æª¢æŸ¥é»
        trainer = PPOTrainer(env, policy, config)
        trainer.load_model(checkpoint_path)
        
        print("âœ… æª¢æŸ¥é»è¼‰å…¥å®Œæˆï¼Œç¹¼çºŒè¨“ç·´...")
        
        # ç¹¼çºŒè¨“ç·´
        trainer.train()
        
    except Exception as e:
        print(f"âŒ çºŒè¨“éç¨‹å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def run_test_episodes(env, wrapper, num_episodes=5):
    """é‹è¡Œæ¸¬è©¦episodes"""
    print(f"\nğŸ¯ é‹è¡Œ {num_episodes} å€‹æ¸¬è©¦episodes...")
    
    all_rewards = []
    all_lengths = []
    
    for episode in range(num_episodes):
        print(f"\n--- Episode {episode + 1}/{num_episodes} ---")
        
        # é‡ç½®ç’°å¢ƒå’Œç­–ç•¥
        state = env.reset()
        wrapper.reset_sequence_cache()
        
        total_reward = 0
        step_count = 0
        
        # é‹è¡Œepisode
        while True:
            # ä½¿ç”¨ç¢ºå®šæ€§å‹•ä½œï¼ˆæ¸¬è©¦æ™‚ï¼‰
            action = wrapper.predict_action(state, deterministic=True)
            
            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, done, info = env.step(action)
            
            total_reward += reward
            step_count += 1
            
            # æ¯100æ­¥é¡¯ç¤ºä¸€æ¬¡é€²åº¦
            if step_count % 100 == 0:
                imu_data = info.get('imu_data', (0, 0))
                roll, pitch = imu_data
                print(f"  æ­¥æ•¸: {step_count}, çå‹µ: {total_reward:.3f}, "
                      f"å§¿æ…‹: roll={roll:.3f}, pitch={pitch:.3f}")
            
            # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
            if done or step_count >= 2000:
                reason = info.get('reason', 'max_steps')
                print(f"  EpisodeçµæŸ: {reason}")
                break
            
            state = next_state
        
        all_rewards.append(total_reward)
        all_lengths.append(step_count)
        
        print(f"  æœ€çµ‚çå‹µ: {total_reward:.3f}")
        print(f"  Episodeé•·åº¦: {step_count}")
    
    # é¡¯ç¤ºçµ±è¨ˆçµæœ
    avg_reward = sum(all_rewards) / len(all_rewards)
    avg_length = sum(all_lengths) / len(all_lengths)

    avg_reward_per_step = avg_reward / avg_length if avg_length > 0 else 0
    
    print(f"\nğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ:")
    print(f"  å¹³å‡çå‹µ: {avg_reward:.3f}")
    print(f"  å¹³å‡æ¯æ­¥çå‹µ: {avg_reward_per_step:.3f}")
    print(f"  å¹³å‡é•·åº¦: {avg_length:.1f}")
    print(f"  æœ€ä½³çå‹µ: {max(all_rewards):.3f}")
    print(f"  æœ€å·®çå‹µ: {min(all_rewards):.3f}")
    
    if avg_reward_per_step >= 0.8:
        print("ğŸ† æ€§èƒ½è©•ç´š: å„ªç§€ (â‰¥0.8)")
    elif avg_reward_per_step >= 0.5:
        print("ğŸ¥ˆ æ€§èƒ½è©•ç´š: è‰¯å¥½ (â‰¥0.5)")
    elif avg_reward_per_step >= 0.2:
        print("ğŸ¥‰ æ€§èƒ½è©•ç´š: ä¸€èˆ¬ (â‰¥0.2)")
    else:
        print("âŒ æ€§èƒ½è©•ç´š: å·® (<0.2)")

def get_training_config():
    """ç²å–è¨“ç·´é…ç½®"""
    from ppo_training_system import PPOConfig
    
    # æª¢æŸ¥æ˜¯å¦æœ‰è‡ªå®šç¾©é…ç½®æª”æ¡ˆ
    config_file = os.path.join(current_dir, "training_config.txt")
    
    if os.path.exists(config_file):
        print(f"ğŸ“ ä½¿ç”¨è‡ªå®šç¾©é…ç½®: {config_file}")
        # é€™è£¡å¯ä»¥å¯¦ç¾å¾æª”æ¡ˆè®€å–é…ç½®çš„é‚è¼¯
        # æš«æ™‚ä½¿ç”¨é è¨­é…ç½®
    
    # æ ¹æ“šä½¿ç”¨éœ€æ±‚é¸æ“‡é…ç½®
    quick_test = check_quick_test_mode()
    
    if quick_test:
        print("âš¡ ä½¿ç”¨å¿«é€Ÿæ¸¬è©¦é…ç½®")
        config = PPOConfig(
            # å¿«é€Ÿæ¸¬è©¦é…ç½®
            total_timesteps=50000,        # 50Kæ­¥
            episodes_per_update=2,        # è¼ƒå°æ‰¹æ¬¡
            update_epochs=2,              # è¼ƒå°‘epochs
            max_episode_steps=1000,       # è¼ƒçŸ­episodes
            eval_frequency=10,            # é »ç¹è©•ä¼°
            save_frequency=20,            # é »ç¹ä¿å­˜
            log_frequency=5,              # é »ç¹æ—¥èªŒ
            
            # ç¶²è·¯é…ç½®
            hidden_size=128,
            n_layer=3,
            n_head=2,
            
            # å­¸ç¿’é…ç½®
            learning_rate=3e-4,
            target_reward=500,            # è¼ƒä½ç›®æ¨™ç”¨æ–¼å¿«é€Ÿæ¸¬è©¦
            
            # æ—¥èªŒé…ç½®
            use_wandb=False,
            run_name=f"hexapod_quick_test_{int(time.time())}"
        )
    else:
        print("ğŸš€ ä½¿ç”¨æ­£å¼è¨“ç·´é…ç½®")
        config = PPOConfig(
            # æ­£å¼è¨“ç·´é…ç½®
            total_timesteps=1000000,      # 1Mæ­¥
            episodes_per_update=4,        # æ¨™æº–æ‰¹æ¬¡
            update_epochs=4,              # æ¨™æº–epochs
            max_episode_steps=2000,       # å®Œæ•´episodes
            eval_frequency=20,            # å®šæœŸè©•ä¼°
            save_frequency=50,            # å®šæœŸä¿å­˜
            log_frequency=10,             # å®šæœŸæ—¥èªŒ
            
            # ç¶²è·¯é…ç½®  
            hidden_size=128,
            n_layer=3,
            n_head=2,
            
            # å­¸ç¿’é…ç½®
            learning_rate=3e-4,
            target_reward=1600,            # é«˜ç›®æ¨™çå‹µ
            
            # æ—¥èªŒé…ç½®
            use_wandb=False,              # å¯è¨­ç‚ºTrueå•Ÿç”¨Weights & Biases
            run_name=f"hexapod_training_{int(time.time())}"
        )
    
    return config

def check_quick_test_mode():
    """æª¢æŸ¥æ˜¯å¦ç‚ºå¿«é€Ÿæ¸¬è©¦æ¨¡å¼"""
    # æª¢æŸ¥å¿«é€Ÿæ¸¬è©¦æ¨™èªŒæª”æ¡ˆ
    quick_test_file = os.path.join(current_dir, "quick_test.flag")
    return os.path.exists(quick_test_file)

def find_best_model():
    """å°‹æ‰¾æœ€ä½³è¨“ç·´æ¨¡å‹"""
    import glob
    
    # âœ… æ›´å…¨é¢çš„æœå°‹è·¯å¾‘
    possible_paths = [
        os.path.join(current_dir, "model_best.pt"),
        os.path.join(current_dir, "runs", "*", "model_best.pt"),
        os.path.join(current_dir, "runs", "*", "model_final.pt"),  # ä¹Ÿå°‹æ‰¾finalæ¨¡å‹
        os.path.join(current_dir, "model_*.pt"),  # ç•¶å‰ç›®éŒ„ä¸­çš„æ‰€æœ‰æ¨¡å‹
    ]
    
    print("ğŸ” æœå°‹å·²è¨“ç·´æ¨¡å‹...")
    
    for path_pattern in possible_paths:
        try:
            print(f"  æª¢æŸ¥è·¯å¾‘: {path_pattern}")
            
            if '*' in path_pattern:
                # âœ… å®‰å…¨çš„ glob æ“ä½œ
                matches = glob.glob(path_pattern)
                if matches:
                    print(f"  æ‰¾åˆ° {len(matches)} å€‹åŒ¹é…æª”æ¡ˆ")
                    # âœ… å®‰å…¨çš„æª”æ¡ˆæ™‚é–“æª¢æŸ¥
                    best_match = None
                    best_time = 0
                    
                    for match in matches:
                        try:
                            mtime = os.path.getmtime(match)
                            if mtime > best_time:
                                best_time = mtime
                                best_match = match
                        except (OSError, PermissionError) as e:
                            print(f"    âš ï¸ ç„¡æ³•è®€å–æª”æ¡ˆæ™‚é–“ {match}: {e}")
                            continue
                    
                    if best_match:
                        print(f"  âœ… é¸æ“‡æœ€æ–°æª”æ¡ˆ: {best_match}")
                        return best_match
            else:
                # âœ… å®‰å…¨çš„æª”æ¡ˆå­˜åœ¨æª¢æŸ¥
                if os.path.exists(path_pattern) and os.path.isfile(path_pattern):
                    print(f"  âœ… æ‰¾åˆ°æª”æ¡ˆ: {path_pattern}")
                    return path_pattern
                    
        except Exception as e:
            print(f"  âš ï¸ æª¢æŸ¥è·¯å¾‘ {path_pattern} æ™‚å‡ºéŒ¯: {e}")
            continue
    
    print("  âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
    return None

def find_latest_checkpoint():
    """å°‹æ‰¾æœ€æ–°çš„æª¢æŸ¥é»"""
    import glob
    runs_dir = os.path.join(current_dir, "runs")
    if not os.path.exists(runs_dir):
        # ä¹Ÿå˜—è©¦åœ¨ç•¶å‰ç›®éŒ„ç›´æ¥å°‹æ‰¾
        current_pattern = os.path.join(current_dir, "model_*.pt")
        checkpoints = glob.glob(current_pattern)
        if checkpoints:
            return max(checkpoints, key=os.path.getmtime)
        return None
    
    # å°‹æ‰¾æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
    pattern = os.path.join(runs_dir, "*", "model_*.pt")
    checkpoints = glob.glob(pattern)
    
    if not checkpoints:
        return None
    
    # è¿”å›æœ€æ–°çš„æª”æ¡ˆ
    return max(checkpoints, key=os.path.getmtime)

def create_mode_files():
    """å‰µå»ºæ¨¡å¼æ§åˆ¶æª”æ¡ˆï¼ˆè¼”åŠ©å·¥å…·ï¼‰"""
    print("\nğŸ”§ å‰µå»ºæ¨¡å¼æ§åˆ¶æª”æ¡ˆ...")
    
    # å‰µå»ºæ¨¡å¼é¸æ“‡æª”æ¡ˆ
    modes = {
        "train": "è¨“ç·´æ–°æ¨¡å‹",
        "test": "æ¸¬è©¦å·²è¨“ç·´æ¨¡å‹", 
        "resume": "çºŒè¨“ç¾æœ‰æ¨¡å‹"
    }
    
    print("é¸æ“‡æ¨¡å¼:")
    for key, desc in modes.items():
        print(f"  {key}: {desc}")
    
    # é è¨­ä½¿ç”¨è¨“ç·´æ¨¡å¼
    selected_mode = "train"
    
    mode_file = os.path.join(current_dir, "run_mode.txt")
    with open(mode_file, 'w') as f:
        f.write(selected_mode)
    
    print(f"âœ… æ¨¡å¼å·²è¨­ç½®ç‚º: {selected_mode}")
    
    # å‰µå»ºå¿«é€Ÿæ¸¬è©¦æ¨™èªŒï¼ˆå¯é¸ï¼‰
    create_quick_test = True  # å¯ä»¥æ”¹ç‚ºFalseä½¿ç”¨æ­£å¼è¨“ç·´
    if create_quick_test:
        quick_test_file = os.path.join(current_dir, "quick_test.flag")
        with open(quick_test_file, 'w') as f:
            f.write("å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")
        print("âš¡ å·²å•Ÿç”¨å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")

def cleanup_on_exit():
    """é€€å‡ºæ™‚çš„æ¸…ç†å·¥ä½œ"""
    print("\nğŸ§¹ æ¸…ç†è³‡æº...")
    
    # é€™è£¡å¯ä»¥æ·»åŠ æ¸…ç†é‚è¼¯
    # ä¾‹å¦‚ä¿å­˜æœ€å¾Œçš„ç‹€æ…‹ã€é—œé–‰æ—¥èªŒç­‰
    
    print("âœ… æ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•¸ï¼šWebotsæ§åˆ¶å™¨å…¥å£é»"""
    print_banner()
    
    try:
        # å‰µå»ºå¿…è¦çš„æ§åˆ¶æª”æ¡ˆ
        create_mode_files()
        
        # ç¢ºå®šé‹è¡Œæ¨¡å¼
        mode = get_run_mode()
        
        print(f"\nğŸ® é‹è¡Œæ¨¡å¼: {mode}")
        
        # æ ¹æ“šæ¨¡å¼åŸ·è¡Œç›¸æ‡‰æ“ä½œ
        success = False
        
        if mode == "train":
            success = train_new_model()
        elif mode == "test":
            success = test_trained_model()
        elif mode == "resume":
            success = resume_training()
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
            print("æ”¯æ´çš„æ¨¡å¼: train, test, resume")
        
        if success:
            print(f"\nğŸ‰ {mode} æ¨¡å¼åŸ·è¡Œå®Œæˆ!")
        else:
            print(f"\nâŒ {mode} æ¨¡å¼åŸ·è¡Œå¤±æ•—!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç¨‹å¼è¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ æœªé æœŸçš„éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup_on_exit()

if __name__ == "__main__":
    main()