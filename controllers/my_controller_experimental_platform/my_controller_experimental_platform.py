"""
å…­è¶³æ©Ÿå™¨äººåœ°å½¢é©æ‡‰æ§åˆ¶ç³»çµ± - PPOè¨“ç·´ç‰ˆæœ¬
ä½¿ç”¨Transformer + PPOåœ¨å‚¾æ–œå¹³å°ä¸Šè¨“ç·´å¹³è¡¡æ§åˆ¶
"""

import sys
import time
import math
import os
import json
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
from transformers import GPT2Config, GPT2Model
from collections import deque
from controller import Supervisor, Robot

class HexapodTransformer(nn.Module):
    """å…­è¶³æ©Ÿå™¨äººåœ°å½¢é©æ‡‰Transformer - Policy Network"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ä½¿ç”¨GPT2æ¶æ§‹ä½œç‚ºbackbone
        transformer_config = GPT2Config(
            vocab_size=1,
            n_positions=config['sequence_length'],
            n_embd=config['hidden_size'],
            n_layer=config['n_layer'],
            n_head=config['n_head']
        )
        
        self.transformer = GPT2Model(transformer_config)
        
        # è¼¸å…¥æŠ•å½±å±¤ (state + action + reward)
        input_dim = config['state_dim'] + config['action_dim'] + config['reward_dim']
        self.input_projection = nn.Linear(input_dim, config['hidden_size'])
        
        # Actorç¶²çµ¡ï¼šè¼¸å‡ºå‹•ä½œå‡å€¼å’Œæ¨™æº–å·®
        self.actor_mean = nn.Linear(config['hidden_size'], config['action_dim'])
        self.actor_logstd = nn.Parameter(torch.zeros(config['action_dim']))
        
        # Criticç¶²çµ¡ï¼šè¼¸å‡ºç‹€æ…‹å€¼
        self.critic = nn.Linear(config['hidden_size'], 1)
        
    def forward(self, state_sequence, action_sequence, reward_sequence):
        batch_size, seq_len = state_sequence.shape[:2]
        
        # çµ„åˆè¼¸å…¥
        combined_input = torch.cat([state_sequence, action_sequence, reward_sequence], dim=-1)
        
        # è¼¸å…¥æŠ•å½±
        embeddings = self.input_projection(combined_input)
        
        # Transformerè™•ç†
        transformer_outputs = self.transformer(inputs_embeds=embeddings)
        last_hidden = transformer_outputs.last_hidden_state[:, -1, :]
        
        return last_hidden
    
    def get_action_and_value(self, state_sequence, action_sequence, reward_sequence, action=None):
        hidden = self.forward(state_sequence, action_sequence, reward_sequence)
        
        # Actorè¼¸å‡º
        action_mean = self.actor_mean(hidden)
        action_logstd = self.actor_logstd.expand_as(action_mean)
        action_std = torch.exp(action_logstd)
        probs = Normal(action_mean, action_std)
        
        if action is None:
            action = probs.sample()
        
        # Criticè¼¸å‡º
        value = self.critic(hidden)
        
        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value

class PPOTrainer:
    """PPOè¨“ç·´å™¨"""
    def __init__(self, agent, config, device):
        self.agent = agent
        self.config = config
        self.device = device  # âœ… æ·»åŠ deviceå±¬æ€§
        self.optimizer = optim.Adam(agent.parameters(), lr=config['learning_rate'])
        
        # è¨“ç·´åƒæ•¸
        self.clip_coef = config.get('clip_coef', 0.2)
        self.vf_coef = config.get('vf_coef', 0.5)
        self.ent_coef = config.get('ent_coef', 0.01)
        self.max_grad_norm = config.get('max_grad_norm', 0.5)
        
        # ç¶“é©—ç·©å­˜
        self.batch_size = config.get('batch_size', 64)
        self.num_epochs = config.get('num_epochs', 4)
        
    def compute_gae(self, rewards, values, dones, next_value, gamma=0.99, gae_lambda=0.95):
        """è¨ˆç®—GAEå„ªå‹¢å‡½æ•¸ - ä¿®æ­£è¨­å‚™åˆ†é…å•é¡Œ"""
        # âœ… ç¢ºä¿æ‰€æœ‰å¼µé‡éƒ½åœ¨åŒä¸€è¨­å‚™ä¸Š
        device = rewards.device
        advantages = torch.zeros_like(rewards).to(device)
        lastgaelam = torch.tensor(0.0).to(device)
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                nextnonterminal = 1.0 - dones[t]
                nextvalues = next_value
            else:
                nextnonterminal = 1.0 - dones[t + 1]
                nextvalues = values[t + 1]
            
            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]
            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam
        
        returns = advantages + values
        return advantages, returns
    
    def update(self, states, actions, rewards, logprobs, values, dones):
        """PPOæ›´æ–° - ä¿®æ­£è¨­å‚™åˆ†é…å•é¡Œ"""
        # âœ… ç¢ºä¿æ‰€æœ‰å¼µé‡éƒ½åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
        next_value = torch.zeros(1).to(self.device)
        
        # å°‡è¼¸å…¥æ•¸æ“šç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
        if isinstance(rewards, np.ndarray):
            rewards = torch.FloatTensor(rewards).to(self.device)
        else:
            rewards = rewards.to(self.device)
            
        if isinstance(logprobs, np.ndarray):
            logprobs = torch.FloatTensor(logprobs).to(self.device)
        else:
            logprobs = logprobs.to(self.device)
            
        if isinstance(values, np.ndarray):
            values = torch.FloatTensor(values).to(self.device)
        else:
            values = values.to(self.device)
            
        if isinstance(dones, np.ndarray):
            dones = torch.FloatTensor(dones).to(self.device)
        else:
            dones = dones.to(self.device)
        
        # è¨ˆç®—å„ªå‹¢å‡½æ•¸å’Œå›å ±
        advantages, returns = self.compute_gae(rewards, values, dones, next_value)
        
        # æ¨™æº–åŒ–å„ªå‹¢
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        batch_size = len(states)
        total_pg_loss = 0
        total_v_loss = 0
        total_entropy_loss = 0
        
        # å¤šè¼ªæ›´æ–°
        for epoch in range(self.num_epochs):
            # âœ… ä¿æŒåºåˆ—é †åºï¼šé€£çºŒå–æ¨£è€Œééš¨æ©Ÿæ‰“äº‚
            for start in range(0, batch_size, self.batch_size):
                end = min(start + self.batch_size, batch_size)
                
                if start >= end:
                    continue
                
                # âœ… ä½¿ç”¨é€£çºŒç´¢å¼•ä¿æŒæ™‚åº
                mb_indices = list(range(start, end))
                
                # é¸æ“‡å°æ‰¹é‡æ•¸æ“šï¼ˆä¿æŒæ™‚é–“é †åºï¼‰
                mb_states = [states[i] for i in mb_indices]
                mb_actions = [actions[i] for i in mb_indices]
                mb_rewards = [rewards[i] for i in mb_indices]
                mb_advantages = advantages[mb_indices]
                mb_returns = returns[mb_indices]
                mb_oldlogprobs = logprobs[mb_indices]
                
                # âœ… æº–å‚™åºåˆ—æ•¸æ“šä¸¦ç¢ºä¿åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
                state_seq = torch.stack([s[0].squeeze(0) for s in mb_states]).to(self.device)
                action_seq = torch.stack([s[1].squeeze(0) for s in mb_states]).to(self.device)
                reward_seq = torch.stack([s[2].squeeze(0) for s in mb_states]).to(self.device)
                mb_actions_tensor = torch.stack([a for a in mb_actions]).to(self.device)
                
                # å‰å‘å‚³æ’­
                _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(
                    state_seq, action_seq, reward_seq, mb_actions_tensor
                )
                
                # PPOæå¤±
                logratio = newlogprob - mb_oldlogprobs
                ratio = logratio.exp()
                
                # Policy loss
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
                
                # Value loss
                v_loss = F.mse_loss(newvalue.flatten(), mb_returns)
                
                # Entropy loss
                entropy_loss = entropy.mean()
                
                # Total loss
                loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef
                
                # æ›´æ–°
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.parameters(), self.max_grad_norm)
                self.optimizer.step()
                
                total_pg_loss += pg_loss.item()
                total_v_loss += v_loss.item()
                total_entropy_loss += entropy_loss.item()
        
        num_updates = self.num_epochs * ((batch_size + self.batch_size - 1) // self.batch_size)
        return {
            'pg_loss': total_pg_loss / num_updates,
            'v_loss': total_v_loss / num_updates,
            'entropy_loss': total_entropy_loss / num_updates
        }

class HexapodExperimentalController:
    """å…­è¶³æ©Ÿå™¨äººå¯¦é©—å¹³å°æ§åˆ¶å™¨ - åŒ…å«PPOè¨“ç·´"""
    
    def __init__(self):
        # åŸºæœ¬è¨­ç½®
        self.MAX_STEPS = 2000
        self.NUM_LEGS = 6
        self.timestep = 20  # Webotsæ™‚é–“æ­¥é•·
        
        # åˆå§‹åŒ–Webots Supervisor
        self.robot = Supervisor()
        self.current_step = 0
        
        # âœ… æ§åˆ¶åƒæ•¸ - åƒè€ƒcontroller2
        self.body_height_offset = 0.5
        self.control_start_step = 100
        self.knee_clamp_positive = True       # è†é—œç¯€é™åˆ¶ç‚ºæ­£å€¼
        self.use_knee_signal_for_ankle = True # è¸é—œç¯€ä½¿ç”¨è†é—œç¯€ä¿¡è™Ÿ
        
        # âœ… åˆå§‹åŒ–è™•ç†éçš„è¨Šè™Ÿè¨˜éŒ„
        self.processed_signals = {}
        for leg_idx in range(1, self.NUM_LEGS + 1):
            self.processed_signals[leg_idx] = {}
            for joint_idx in range(1, 4):  # 1:é«–, 2:è†, 3:è¸
                self.processed_signals[leg_idx][joint_idx] = np.zeros(self.MAX_STEPS + 1)
        
        # Transformeré…ç½®
        self.transformer_config = {
            'sequence_length': 50,
            'state_dim': 6,
            'action_dim': 6,  # åªæ§åˆ¶6å€‹è†é—œç¯€
            'reward_dim': 1,
            'hidden_size': 128,
            'n_layer': 3,
            'n_head': 2,
            'learning_rate': 3e-4,
            'batch_size': 32,
            'num_epochs': 4,
            'clip_coef': 0.2,
            'vf_coef': 0.5,
            'ent_coef': 0.01,
            'max_grad_norm': 0.5
        }
        
        # åˆå§‹åŒ–ç¥ç¶“ç¶²çµ¡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.agent = HexapodTransformer(self.transformer_config).to(self.device)
        self.trainer = PPOTrainer(self.agent, self.transformer_config, self.device)  # âœ… å‚³édevice
        
        # åºåˆ—ç·©å­˜
        self.state_buffer = deque(maxlen=self.transformer_config['sequence_length'])
        self.action_buffer = deque(maxlen=self.transformer_config['sequence_length'])
        self.reward_buffer = deque(maxlen=self.transformer_config['sequence_length'])
        
        # è¨“ç·´æ•¸æ“šæ”¶é›†
        self.episode_states = []
        self.episode_actions = []
        self.episode_rewards = []
        self.episode_logprobs = []
        self.episode_values = []
        self.episode_dones = []
        
        # åˆå§‹åŒ–å„ç¨®çµ„ä»¶
        self.init_motors()
        self.init_sensors()
        self.init_platform_control()
        self._initialize_buffers()
        self.create_output_directories()
        
        # è¨“ç·´çµ±è¨ˆ
        self.episode_count = 0
        self.total_steps = 0
        self.best_reward = -float('inf')
        
        # çµ‚æ­¢ç›¸é—œ
        self.episode_terminated = False
        self.termination_reason = None
        
        print("å…­è¶³æ©Ÿå™¨äººå¯¦é©—å¹³å°æ§åˆ¶å™¨å·²åˆå§‹åŒ–")
        print(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        print(f"æ§åˆ¶é »ç‡: {1000/self.timestep} Hz")
    
    def init_motors(self):
        """åˆå§‹åŒ–æ‰€æœ‰é¦¬é”"""
        leg_mapping = {
            1: ('R0', 'å³å‰è…¿'), 2: ('R1', 'å³ä¸­è…¿'), 3: ('R2', 'å³å¾Œè…¿'),
            4: ('L2', 'å·¦å¾Œè…¿'), 5: ('L1', 'å·¦ä¸­è…¿'), 6: ('L0', 'å·¦å‰è…¿')
        }
        
        joint_names = ['0', '1', '2']
        joint_descriptions = ['é«–é—œç¯€', 'è†é—œç¯€', 'è¸é—œç¯€']
        
        self.motors = {}
        
        print("=== é¦¬é”åˆå§‹åŒ– ===")
        for leg_idx in range(1, self.NUM_LEGS + 1):
            if leg_idx not in leg_mapping:
                continue
                
            leg_name, leg_desc = leg_mapping[leg_idx]
            self.motors[leg_idx] = {}
            
            for j, joint_name in enumerate(joint_names):
                joint_idx = j + 1
                motor_name = f"{leg_name}{joint_name}"
                joint_desc = joint_descriptions[j]
                
                try:
                    motor = self.robot.getDevice(motor_name)
                    if motor is None:
                        print(f"  âš ï¸  æ‰¾ä¸åˆ°é¦¬é” {motor_name}")
                        continue
                    
                    motor.setPosition(0.0)
                    motor.setVelocity(motor.getMaxVelocity())
                    
                    self.motors[leg_idx][joint_idx] = motor
                    print(f"  âœ“  è…¿{leg_idx} {joint_desc} -> {motor_name}")
                    
                except Exception as e:
                    print(f"  âŒ åˆå§‹åŒ–é¦¬é” {motor_name} å¤±æ•—: {e}")
    
    def init_sensors(self):
        """åˆå§‹åŒ–æ„Ÿæ¸¬å™¨"""
        # IMUæ„Ÿæ¸¬å™¨
        try:
            self.imu_device = self.robot.getDevice("inertialunit1")
            if self.imu_device is None:
                print("âŒ æ‰¾ä¸åˆ°IMUæ„Ÿæ¸¬å™¨")
                return
            self.imu_device.enable(self.timestep)
            print("âœ… IMUæ„Ÿæ¸¬å™¨å·²å•Ÿç”¨")
        except Exception as e:
            print(f"âŒ IMUåˆå§‹åŒ–å¤±æ•—: {e}")
            self.imu_device = None
        
        # GPSæ„Ÿæ¸¬å™¨ï¼ˆç”¨æ–¼ä½ç½®è¿½è¹¤ï¼‰
        try:
            self.gps_device = self.robot.getDevice("gps")
            if self.gps_device is None:
                print("âš ï¸ æœªæ‰¾åˆ°GPSæ„Ÿæ¸¬å™¨ï¼Œå°‡ä½¿ç”¨Supervisorç¯€é»")
                self.gps_device = None
            else:
                self.gps_device.enable(self.timestep)
                print("âœ… GPSæ„Ÿæ¸¬å™¨å·²å•Ÿç”¨")
        except Exception as e:
            print(f"âŒ GPSåˆå§‹åŒ–å¤±æ•—: {e}")
            self.gps_device = None
    
    def init_platform_control(self):
        """åˆå§‹åŒ–å¯¦é©—å¹³å°æ§åˆ¶"""
        try:
            # ç²å–experimental_platformç¯€é»
            self.platform_node = self.robot.getFromDef("experimental_platform")
            if self.platform_node is None:
                print("âŒ æ‰¾ä¸åˆ°experimental_platformç¯€é»")
                return
            
            # ç²å–platform_motor
            children_field = self.platform_node.getField("children")
            children_count = children_field.getCount()
            
            self.platform_motor_joint = None
            for i in range(children_count):
                child = children_field.getMFNode(i)
                if child is not None and child.getDef() == "platform_motor":
                    self.platform_motor_joint = child
                    break
            
            if self.platform_motor_joint is None:
                print("âŒ æ‰¾ä¸åˆ°platform_motor")
                return
            
            # ç²å–positionæ§åˆ¶æ¥å£
            joint_params_field = self.platform_motor_joint.getField("jointParameters")
            joint_params_node = joint_params_field.getSFNode()
            self.platform_position_field = joint_params_node.getField("position")
            
            print("âœ… å¯¦é©—å¹³å°æ§åˆ¶å·²åˆå§‹åŒ–")
            
        except Exception as e:
            print(f"âŒ å¹³å°æ§åˆ¶åˆå§‹åŒ–å¤±æ•—: {e}")
            self.platform_position_field = None
    
    def _initialize_buffers(self):
        """åˆå§‹åŒ–åºåˆ—ç·©å­˜"""
        seq_len = self.transformer_config['sequence_length']
        for _ in range(seq_len):
            self.state_buffer.append(np.zeros(6))
            self.action_buffer.append(np.zeros(6))
            self.reward_buffer.append(np.array([0.0]))
    
    def create_output_directories(self):
        """å»ºç«‹è¼¸å‡ºç›®éŒ„"""
        self.output_dir = "experimental_platform_training"
        self.models_dir = os.path.join(self.output_dir, "models")
        self.logs_dir = os.path.join(self.output_dir, "logs")
        
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.logs_dir, exist_ok=True)
    
    def control_platform(self):
        """æ§åˆ¶å¯¦é©—å¹³å°é‹å‹•"""
        if self.platform_position_field is None:
            return
        
        current_time = self.robot.getTime()
        # æ­£å¼¦æ³¢é‹å‹•ï¼š0.2 * sin(Ï€ * t)
        sine_angle = 0.2 * math.sin(1.0 * math.pi * current_time)
        self.platform_position_field.setSFFloat(sine_angle)
    
    def get_robot_position(self):
        """ç²å–æ©Ÿå™¨äººä½ç½®"""
        try:
            if self.gps_device is not None:
                return np.array(self.gps_device.getValues())
            else:
                # ä½¿ç”¨Supervisorç²å–ä½ç½®
                robot_node = self.robot.getSelf()
                if robot_node:
                    position = robot_node.getPosition()
                    return np.array(position)
                return np.array([0, 0, 0])
        except:
            return np.array([0, 0, 0])
    
    def get_imu_data(self):
        """ç²å–IMUæ•¸æ“šä¸¦è½‰æ›ç‚º6ç¶­è…³éƒ¨æ–¹å‘åˆ†é‡"""
        if self.imu_device is None:
            return np.zeros(6)
        
        try:
            roll_pitch_yaw = self.imu_device.getRollPitchYaw()
            pitch, roll, yaw = roll_pitch_yaw[1], roll_pitch_yaw[0], roll_pitch_yaw[2]
            
            # è¨ˆç®—6ç¶­è…³éƒ¨æ–¹å‘åˆ†é‡
            sqrt_half = math.sqrt(0.5)
            e1 = (pitch + roll) * sqrt_half  # å‰å³è…³
            e2 = roll                        # å³ä¸­è…³
            e3 = (-pitch + roll) * sqrt_half # å¾Œå³è…³
            e4 = (-pitch - roll) * sqrt_half # å¾Œå·¦è…³
            e5 = -roll                       # å·¦ä¸­è…³
            e6 = (pitch - roll) * sqrt_half  # å‰å·¦è…³
            
            return np.array([e1, e2, e3, e4, e5, e6])
            
        except Exception as e:
            print(f"IMUè®€å–éŒ¯èª¤: {e}")
            return np.zeros(6)
    
    def get_raw_imu_data(self):
        """ç²å–åŸå§‹IMUæ•¸æ“šï¼ˆç”¨æ–¼çå‹µè¨ˆç®—ï¼‰"""
        if self.imu_device is None:
            return np.array([0, 0, 0])
        
        try:
            roll_pitch_yaw = self.imu_device.getRollPitchYaw()
            return np.array([roll_pitch_yaw[0], roll_pitch_yaw[1], roll_pitch_yaw[2]])  # roll, pitch, yaw
        except:
            return np.array([0, 0, 0])
    
    def calculate_reward(self, raw_imu_data):
        """è¨ˆç®—çå‹µå‡½æ•¸"""
        roll, pitch, yaw = raw_imu_data
        
        # ç©©å®šæ€§çå‹µ
        stability_reward = math.exp(-((abs(pitch) + abs(roll)) / 2) ** 2 / (0.1 ** 2))
        
        # è·Œå€’æ‡²ç½°
        fall_penalty = 0
        if abs(pitch) >= 0.524 or abs(roll) >= 0.524:
            fall_penalty = -1
        
        # ä½ç½®æ‡²ç½°ï¼ˆæª¢æŸ¥æ˜¯å¦è¶…å‡ºå¹³å°ç¯„åœï¼‰
        position = self.get_robot_position()
        position_penalty = 0
        if not (-0.2 < position[0] < 0.2 and -0.18 < position[1] < 0.18):
            position_penalty = -1
        
        total_reward = stability_reward + fall_penalty + position_penalty
        return total_reward
    
    def check_termination_conditions(self, raw_imu_data):
        """æª¢æŸ¥çµ‚æ­¢æ¢ä»¶"""
        roll, pitch, yaw = raw_imu_data
        
        # è§’åº¦éå¤§
        if abs(pitch) > 0.524 or abs(roll) > 0.524:
            self.episode_terminated = True
            self.termination_reason = "è§’åº¦éå¤§"
            return True
        
        # è¶…å‡ºå¹³å°ç¯„åœ
        position = self.get_robot_position()
        if not (-0.2 < position[0] < 0.2 and -0.18 < position[1] < 0.18):
            self.episode_terminated = True
            self.termination_reason = "è¶…å‡ºå¹³å°"
            return True
        
        # é”åˆ°æœ€å¤§æ­¥æ•¸
        if self.current_step >= self.MAX_STEPS:
            self.episode_terminated = True
            self.termination_reason = "é”åˆ°æœ€å¤§æ­¥æ•¸"
            return True
        
        return False
    
    def get_transformer_action(self):
        """ç²å–Transformerå‹•ä½œ - ä¿®æ­£è¨­å‚™åˆ†é…å•é¡Œ"""
        if len(self.state_buffer) < self.transformer_config['sequence_length']:
            return np.zeros(6)
        
        try:
            # âœ… ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
            state_seq = torch.FloatTensor(np.array(list(self.state_buffer))).unsqueeze(0).to(self.device)
            action_seq = torch.FloatTensor(np.array(list(self.action_buffer))).unsqueeze(0).to(self.device)
            reward_seq = torch.FloatTensor(np.array(list(self.reward_buffer))).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action, logprob, entropy, value = self.agent.get_action_and_value(
                    state_seq, action_seq, reward_seq
                )
            
            # âœ… ç¢ºä¿å„²å­˜çš„è¨“ç·´æ•¸æ“šåœ¨CPUä¸Šï¼ˆç¯€çœGPUå…§å­˜ï¼‰
            self.episode_states.append((state_seq.cpu(), action_seq.cpu(), reward_seq.cpu()))
            self.episode_actions.append(action.cpu())
            self.episode_logprobs.append(logprob.cpu())
            self.episode_values.append(value.cpu())
            
            return action.cpu().numpy().flatten()
            
        except Exception as e:
            print(f"Transformeræ¨ç†éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return np.zeros(6)
    
    def process_special_joints(self, motor_angle, leg_idx, joint_idx):
        """è™•ç†ç‰¹æ®Šé—œç¯€ - åƒè€ƒcontroller2"""
        # è†é—œç¯€è™•ç†ï¼šç¢ºä¿æ­£å€¼ï¼ˆç«™ç«‹å§¿æ…‹ï¼‰
        if joint_idx == 2:
            if self.knee_clamp_positive and motor_angle <= 0:
                return 0.0
        
        # è¸é—œç¯€ç‰¹æ®Šè™•ç†ï¼šç‰¹å®šè…¿éƒ¨çš„è² å€¼é™åˆ¶
        elif joint_idx == 3:
            if not self.use_knee_signal_for_ankle:
                if (leg_idx == 2 or leg_idx == 5) and motor_angle >= 0:  # R1, L1ä¸­è…¿
                    return 0.0
        
        return motor_angle
    
    def adjust_signal_direction(self, motor_angle, leg_idx, joint_idx):
        """èª¿æ•´è¨Šè™Ÿæ–¹å‘ - åƒè€ƒcontroller2
        
        åå‘è¦å‰‡ï¼š
        - å³å´è…¿éƒ¨(leg_idx 1-3): è¸é—œç¯€åå‘
        - å·¦å´è…¿éƒ¨(leg_idx 4-6): é«–é—œç¯€å’Œè†é—œç¯€åå‘  
        - é¡å¤–è¸é—œç¯€åå‘: R0(leg_idx=1), L0(leg_idx=6), R1(leg_idx=2), L1(leg_idx=5)
        """
        
        # å³å´è…¿éƒ¨è¸é—œç¯€åå‘ï¼ˆç•¶ä¸ä½¿ç”¨è†é—œç¯€ä¿¡è™Ÿæ§åˆ¶è¸é—œç¯€æ™‚ï¼‰
        if not self.use_knee_signal_for_ankle:
            if leg_idx <= 3 and joint_idx == 3:
                motor_angle = -motor_angle
        
        # å·¦å´è…¿éƒ¨é«–é—œç¯€å’Œè†é—œç¯€åå‘
        if leg_idx >= 4 and (joint_idx == 1 or joint_idx == 2):
            motor_angle = -motor_angle
        
        # é¡å¤–çš„è¸é—œç¯€åå‘ï¼ˆç•¶ä¸ä½¿ç”¨è†é—œç¯€ä¿¡è™Ÿæ§åˆ¶è¸é—œç¯€æ™‚ï¼‰
        if not self.use_knee_signal_for_ankle:
            if (leg_idx == 1 or leg_idx == 6) and joint_idx == 3:  # R0, L0å‰è…¿
                motor_angle = -motor_angle
            if (leg_idx == 2 or leg_idx == 5) and joint_idx == 3:  # R1, L1ä¸­è…¿
                motor_angle = -motor_angle

        return motor_angle
    
    def replace_ankle_with_knee_signal(self, motor_angle, leg_idx, joint_idx):
        """å°‡è¸é—œç¯€è¨Šè™Ÿæ›¿æ›ç‚ºè†é—œç¯€è¨Šè™Ÿ - åƒè€ƒcontroller2"""
        if joint_idx == 3 and self.use_knee_signal_for_ankle:
            # ä½¿ç”¨åŒéš»è…³è†é—œç¯€çš„è™•ç†éè¨Šè™Ÿ
            if leg_idx in self.processed_signals and 2 in self.processed_signals[leg_idx]:
                knee_signal = self.processed_signals[leg_idx][2][self.current_step]
                return knee_signal * 1  # å¯ä»¥èª¿æ•´ä¿‚æ•¸
            else:
                # å¦‚æœæ²’æœ‰è†é—œç¯€ä¿¡è™Ÿï¼Œä½¿ç”¨åŸºç¤è¸é—œç¯€è§’åº¦
                return motor_angle
        return motor_angle
    
    def apply_height_offset(self, motor_angle, leg_idx, joint_idx):
        """æ‡‰ç”¨æ©Ÿèº«é«˜åº¦åç§» - åƒè€ƒcontroller2"""
        # åªå°è†é—œç¯€ï¼Œä»¥åŠåœ¨ä¸ä½¿ç”¨è†é—œç¯€ä¿¡è™Ÿæ§åˆ¶è¸é—œç¯€æ™‚çš„è¸é—œç¯€æ‡‰ç”¨åç§»
        should_apply_offset = (
            joint_idx == 2 or  # è†é—œç¯€
            (joint_idx == 3 and not self.use_knee_signal_for_ankle)  # è¸é—œç¯€(æ¢ä»¶æ€§)
        )
    
        if should_apply_offset:
            # å³å´è…¿éƒ¨(1-3)ç”¨è² åç§»ï¼Œå·¦å´è…¿éƒ¨(4-6)ç”¨æ­£åç§»
            offset_direction = -1 if leg_idx <= 3 else 1
            return motor_angle + (self.body_height_offset * offset_direction)
        
        return motor_angle
    
    def apply_fixed_angles_with_corrections(self, corrections):
        """æ‡‰ç”¨å›ºå®šè§’åº¦å’ŒTransformerä¿®æ­£ - ä½¿ç”¨controller2çš„è™•ç†æµç¨‹"""
        # åŸºç¤å›ºå®šè§’åº¦
        base_angles = {
            'hip': 0.0,      # é«–é—œç¯€å›ºå®šç‚º0
            'knee': 0.0,     # è†é—œç¯€åŸºç¤è§’åº¦
            'ankle': 0.0     # è¸é—œç¯€åŸºç¤è§’åº¦
        }
        
        # åˆå§‹åŒ–è™•ç†éçš„è¨Šè™Ÿè¨˜éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not hasattr(self, 'processed_signals'):
            self.processed_signals = {}
            for leg_idx in range(1, self.NUM_LEGS + 1):
                self.processed_signals[leg_idx] = {}
                for joint_idx in range(1, 4):
                    self.processed_signals[leg_idx][joint_idx] = np.zeros(self.MAX_STEPS + 1)
        
        for leg_idx in range(1, self.NUM_LEGS + 1):
            if leg_idx not in self.motors:
                continue
            
            for joint_idx in range(1, 4):  # 1:é«–, 2:è†, 3:è¸
                if joint_idx not in self.motors[leg_idx]:
                    continue
                
                # æ­¥é©Ÿ1: ç²å–åŸºç¤è§’åº¦
                if joint_idx == 1:  # é«–é—œç¯€
                    motor_angle = base_angles['hip']
                elif joint_idx == 2:  # è†é—œç¯€
                    motor_angle = base_angles['knee']
                else:  # è¸é—œç¯€
                    motor_angle = base_angles['ankle']
                
                # æ­¥é©Ÿ2: è¸é—œç¯€è¨Šè™Ÿæ›¿æ›ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                motor_angle = self.replace_ankle_with_knee_signal(motor_angle, leg_idx, joint_idx)
                
                # æ­¥é©Ÿ3: ç‰¹æ®Šé—œç¯€è™•ç†
                motor_angle = self.process_special_joints(motor_angle, leg_idx, joint_idx)
                
                # æ­¥é©Ÿ4: è¨Šè™Ÿæ–¹å‘èª¿æ•´ï¼ˆé¦¬é”è½‰å‹•æ–¹å‘ç›¸é—œï¼‰
                motor_angle = self.adjust_signal_direction(motor_angle, leg_idx, joint_idx)
                
                # æ­¥é©Ÿ5: æ‡‰ç”¨æ©Ÿèº«é«˜åº¦åç§»
                motor_angle = self.apply_height_offset(motor_angle, leg_idx, joint_idx)
                
                # æ­¥é©Ÿ6: æ·»åŠ Transformerä¿®æ­£é‡ï¼ˆåªå°è†é—œç¯€ï¼‰
                if joint_idx == 2:  # åªæœ‰è†é—œç¯€æ¥å—Transformerä¿®æ­£
                    correction_idx = leg_idx - 1  # 6å€‹è†é—œç¯€å°æ‡‰6å€‹ä¿®æ­£é‡
                    if correction_idx < len(corrections):
                        motor_angle += corrections[correction_idx]
                        if self.current_step % 200 == 0:
                            print(f"è…¿{leg_idx}è†é—œç¯€ä¿®æ­£é‡: {corrections[correction_idx]:.4f}")
                
                # æ­¥é©Ÿ7: å„²å­˜è™•ç†éçš„è¨Šè™Ÿ
                self.processed_signals[leg_idx][joint_idx][self.current_step] = motor_angle
                
                # æ­¥é©Ÿ8: ç™¼é€åˆ°é¦¬é”
                try:
                    if self.current_step >= self.control_start_step:
                        self.motors[leg_idx][joint_idx].setPosition(motor_angle)
                except Exception as e:
                    print(f"é¦¬é”æ§åˆ¶éŒ¯èª¤ (è…¿{leg_idx}, é—œç¯€{joint_idx}): {e}")
    
    def save_training_checkpoint(self):
        """ä¿å­˜è¨“ç·´æª¢æŸ¥é»"""
        checkpoint = {
            'episode': self.episode_count,
            'total_steps': self.total_steps,
            'model_state_dict': self.agent.state_dict(),
            'optimizer_state_dict': self.trainer.optimizer.state_dict(),
            'best_reward': self.best_reward,
            'config': self.transformer_config
        }
        
        checkpoint_path = os.path.join(self.models_dir, f"checkpoint_episode_{self.episode_count}.pt")
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if hasattr(self, 'episode_total_reward') and self.episode_total_reward > self.best_reward:
            self.best_reward = self.episode_total_reward
            best_model_path = os.path.join(self.models_dir, "best_model.pt")
            torch.save(checkpoint, best_model_path)
    
    def load_training_checkpoint(self, checkpoint_path):
        """è¼‰å…¥è¨“ç·´æª¢æŸ¥é»"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.agent.load_state_dict(checkpoint['model_state_dict'])
            self.trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.episode_count = checkpoint['episode']
            self.total_steps = checkpoint['total_steps']
            self.best_reward = checkpoint['best_reward']
            print(f"âœ… è¼‰å…¥æª¢æŸ¥é»ï¼šEpisode {self.episode_count}")
        except Exception as e:
            print(f"âŒ è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")
    
    def save_episode_data(self):
        """ä¿å­˜episodeè¨“ç·´æ•¸æ“šåˆ°æª”æ¡ˆï¼ˆé‡ç½®å‰ï¼‰"""
        episode_data = {
            'episode_count': self.episode_count,
            'total_steps': self.total_steps,
            'episode_total_reward': getattr(self, 'episode_total_reward', 0),
            'termination_reason': self.termination_reason,
            'current_step': self.current_step,
            'best_reward': self.best_reward,
            # æ·±æ‹·è²è¨“ç·´æ•¸æ“šä»¥é¿å…å¼•ç”¨å•é¡Œ
            'states': [s for s in self.episode_states] if self.episode_states else [],
            'actions': [a.clone() if hasattr(a, 'clone') else a for a in self.episode_actions],
            'rewards': self.episode_rewards.copy() if self.episode_rewards else [],
            'logprobs': [lp.clone() if hasattr(lp, 'clone') else lp for lp in self.episode_logprobs],
            'values': [v.clone() if hasattr(v, 'clone') else v for v in self.episode_values],
            'dones': self.episode_dones.copy() if self.episode_dones else [],
            # ä¿å­˜ç·©å­˜ç‹€æ…‹
            'state_buffer': list(self.state_buffer),
            'action_buffer': list(self.action_buffer),
            'reward_buffer': list(self.reward_buffer)
        }
        
        # ä¿å­˜åˆ°æª”æ¡ˆ
        episode_file = os.path.join(self.logs_dir, "pending_episode_data.pkl")
        try:
            with open(episode_file, 'wb') as f:
                pickle.dump(episode_data, f)
            print(f"âœ… Episodeæ•¸æ“šå·²ä¿å­˜åˆ°æª”æ¡ˆ")
        except Exception as e:
            print(f"âŒ ä¿å­˜episodeæ•¸æ“šå¤±æ•—: {e}")
    
    def load_and_process_episode_data(self):
        """è¼‰å…¥ä¸¦è™•ç†episodeè¨“ç·´æ•¸æ“šï¼ˆé‡ç½®å¾Œç«‹å³èª¿ç”¨ï¼‰"""
        episode_file = os.path.join(self.logs_dir, "pending_episode_data.pkl")
        
        if not os.path.exists(episode_file):
            return False
        
        try:
            with open(episode_file, 'rb') as f:
                episode_data = pickle.load(f)
            
            print(f"âœ… è¼‰å…¥å¾…è™•ç†Episodeæ•¸æ“š")
            print(f"   Episode: {episode_data.get('episode_count', 0)}")
            print(f"   ç¸½çå‹µ: {episode_data.get('episode_total_reward', 0):.4f}")
            print(f"   çµ‚æ­¢åŸå› : {episode_data.get('termination_reason', 'æœªçŸ¥')}")
            print(f"   æ­¥æ•¸: {episode_data.get('current_step', 0)}")
            
            # æ¢å¾©è¨“ç·´çµ±è¨ˆï¼ˆä½†ä¸æ˜¯ç•¶å‰episodeç‹€æ…‹ï¼‰
            self.total_steps = episode_data.get('total_steps', 0)
            self.best_reward = episode_data.get('best_reward', -float('inf'))
            
            # åŸ·è¡ŒPPOè¨“ç·´ï¼ˆå¦‚æœæœ‰è¶³å¤ æ•¸æ“šï¼‰
            states = episode_data.get('states', [])
            actions = episode_data.get('actions', [])
            rewards = episode_data.get('rewards', [])
            logprobs = episode_data.get('logprobs', [])
            values = episode_data.get('values', [])
            dones = episode_data.get('dones', [])
            
            if len(states) >= 10:  # éœ€è¦è¶³å¤ çš„æ•¸æ“šé€²è¡Œè¨“ç·´
                print("åŸ·è¡Œå»¶é²çš„PPOè¨“ç·´...")
                try:
                    # è½‰æ›æ•¸æ“šæ ¼å¼
                    rewards_tensor = torch.tensor(rewards)
                    logprobs_tensor = torch.stack(logprobs) if logprobs else torch.tensor([])
                    values_tensor = torch.stack(values).squeeze() if values else torch.tensor([])
                    dones_tensor = torch.tensor(dones)
                    
                    # åŸ·è¡ŒPPOæ›´æ–°
                    loss_info = self.trainer.update(states, actions, rewards_tensor, 
                                                  logprobs_tensor, values_tensor, dones_tensor)
                    
                    print(f"PPOæ›´æ–°å®Œæˆ - PGæå¤±: {loss_info['pg_loss']:.4f}, "
                          f"Væå¤±: {loss_info['v_loss']:.4f}, "
                          f"ç†µæå¤±: {loss_info['entropy_loss']:.4f}")
                    
                    # æ›´æ–°æœ€ä½³çå‹µ
                    episode_reward = episode_data.get('episode_total_reward', 0)
                    if episode_reward > self.best_reward:
                        self.best_reward = episode_reward
                        self.save_training_checkpoint()  # ä¿å­˜æœ€ä½³æ¨¡å‹
                    
                except Exception as e:
                    print(f"âŒ PPOè¨“ç·´å¤±æ•—: {e}")
            else:
                print(f"âš ï¸ æ•¸æ“šä¸è¶³ï¼Œè·³éPPOè¨“ç·´ (æ•¸æ“šé‡: {len(states)})")
            
            # æ¸…ç†episodeæ•¸æ“šæª”æ¡ˆ
            os.remove(episode_file)
            print("âœ… Episodeæ•¸æ“šæª”æ¡ˆå·²æ¸…ç†")
            
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥episodeæ•¸æ“šå¤±æ•—: {e}")
            # å˜—è©¦æ¸…ç†æå£çš„æª”æ¡ˆ
            try:
                os.remove(episode_file)
                print("ğŸ—‘ï¸ å·²æ¸…ç†æå£çš„æ•¸æ“šæª”æ¡ˆ")
            except:
                pass
            return False
    
    def reset_episode(self):
        """é‡ç½®episode - ä½¿ç”¨simulationReset()é¿å…æ§åˆ¶å™¨ä¸­æ–·"""
        print(f"\n=== Episode {self.episode_count} çµæŸ ===")
        if hasattr(self, 'episode_total_reward'):
            print(f"ç¸½çå‹µ: {self.episode_total_reward:.4f}")
        if self.termination_reason:
            print(f"çµ‚æ­¢åŸå› : {self.termination_reason}")
        print(f"æ­¥æ•¸: {self.current_step}")
        
        # âœ… å…ˆåŸ·è¡ŒPPOè¨“ç·´ï¼ˆåœ¨é‡ç½®å‰ï¼‰
        if len(self.episode_states) >= 10:  # ç¢ºä¿æœ‰è¶³å¤ æ•¸æ“š
            print("åŸ·è¡ŒPPOè¨“ç·´...")
            self.train_ppo()
        else:
            print(f"âš ï¸ æ•¸æ“šä¸è¶³ï¼Œè·³éPPOè¨“ç·´ (æ•¸æ“šé‡: {len(self.episode_states)})")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if hasattr(self, 'episode_total_reward') and self.episode_total_reward > self.best_reward:
            self.best_reward = self.episode_total_reward
            self.save_training_checkpoint()
            print(f"ğŸ† æ–°çš„æœ€ä½³çå‹µ: {self.best_reward:.4f}")
        
        # âœ… ä½¿ç”¨simulationReset()é‡ç½®ç’°å¢ƒ
        print("é‡ç½®æ¨¡æ“¬ç’°å¢ƒ...")
        try:
            self.robot.simulationReset()
            print("âœ… æ¨¡æ“¬ç’°å¢ƒé‡ç½®æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡æ“¬ç’°å¢ƒé‡ç½®å¤±æ•—: {e}")
        
        # æ›´æ–°episodeè¨ˆæ•¸
        self.episode_count += 1
        
        # é‡ç½®ç•¶å‰episodeç‹€æ…‹
        self.current_step = 0
        self.episode_terminated = False
        self.termination_reason = None
        self.episode_total_reward = 0  # é‡ç½®ç•¶å‰episodeçå‹µ
        
        # æ¸…ç©ºç•¶å‰episodeæ•¸æ“š
        self.episode_states.clear()
        self.episode_actions.clear()
        self.episode_rewards.clear()
        self.episode_logprobs.clear()
        self.episode_values.clear()
        self.episode_dones.clear()
        
        # é‡æ–°åˆå§‹åŒ–ç·©å­˜
        self._initialize_buffers()
        
        print(f"é–‹å§‹ Episode {self.episode_count}")
        
        # å®šæœŸä¿å­˜æª¢æŸ¥é»
        if self.episode_count % 10 == 0:
            self.save_training_checkpoint()
            print(f"ğŸ“ å·²ä¿å­˜æª¢æŸ¥é» (Episode {self.episode_count})")
    
    def train_ppo(self):
        """åŸ·è¡ŒPPOè¨“ç·´ - ä¿®æ­£è¨­å‚™åˆ†é…å•é¡Œ"""
        if len(self.episode_states) < 10:  # éœ€è¦è¶³å¤ çš„æ•¸æ“š
            print(f"âš ï¸ è¨“ç·´æ•¸æ“šä¸è¶³: {len(self.episode_states)}")
            return
        
        try:
            # âœ… è½‰æ›æ•¸æ“šæ ¼å¼ä¸¦ç¢ºä¿è¨­å‚™ä¸€è‡´æ€§
            states = self.episode_states
            actions = self.episode_actions
            rewards = torch.FloatTensor(self.episode_rewards).to(self.device)
            
            # è™•ç†logprobså’Œvalues
            if len(self.episode_logprobs) > 0:
                logprobs = torch.stack([lp.to(self.device) for lp in self.episode_logprobs])
            else:
                logprobs = torch.zeros(len(states)).to(self.device)
            
            if len(self.episode_values) > 0:
                values = torch.stack([v.to(self.device) for v in self.episode_values]).squeeze()
            else:
                values = torch.zeros(len(states)).to(self.device)
            
            # âœ… ç¢ºä¿donesç¶­åº¦èˆ‡å…¶ä»–å¼µé‡åŒ¹é…
            dones_list = self.episode_dones
            if len(dones_list) > len(states):
                dones_list = dones_list[:len(states)]  # æˆªæ–·åˆ°æ­£ç¢ºé•·åº¦
            elif len(dones_list) < len(states):
                # è£œé½Šåˆ°æ­£ç¢ºé•·åº¦ï¼ˆç”¨0å¡«å……ï¼‰
                dones_list.extend([0.0] * (len(states) - len(dones_list)))
            
            dones = torch.FloatTensor(dones_list).to(self.device)
            
            # ç¢ºä¿ç¶­åº¦ä¸€è‡´
            if len(values.shape) == 0:
                values = values.unsqueeze(0)
            if len(logprobs.shape) == 0:
                logprobs = logprobs.unsqueeze(0)
            
            # âœ… ç¢ºä¿æ‰€æœ‰å¼µé‡é•·åº¦ä¸€è‡´
            min_length = min(len(states), len(actions), rewards.shape[0], 
                           logprobs.shape[0], values.shape[0], dones.shape[0])
            
            if min_length < len(states):
                states = states[:min_length]
                actions = actions[:min_length]
                rewards = rewards[:min_length]
                logprobs = logprobs[:min_length]
                values = values[:min_length]
                dones = dones[:min_length]
            
            print(f"è¨“ç·´æ•¸æ“šç¶­åº¦æª¢æŸ¥:")
            print(f"  states: {len(states)}")
            print(f"  actions: {len(actions)}")
            print(f"  rewards: {rewards.shape}, device: {rewards.device}")
            print(f"  logprobs: {logprobs.shape}, device: {logprobs.device}")
            print(f"  values: {values.shape}, device: {values.device}")
            print(f"  dones: {dones.shape}, device: {dones.device}")
            
            # åŸ·è¡ŒPPOæ›´æ–°
            loss_info = self.trainer.update(states, actions, rewards, logprobs, values, dones)
            
            print(f"PPOæ›´æ–°å®Œæˆ - PGæå¤±: {loss_info['pg_loss']:.4f}, "
                  f"Væå¤±: {loss_info['v_loss']:.4f}, "
                  f"ç†µæå¤±: {loss_info['entropy_loss']:.4f}")
            
        except Exception as e:
            print(f"PPOè¨“ç·´éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
    
    def run(self):
        """ä¸»é‹è¡Œå¾ªç’°"""
        print("é–‹å§‹PPOè¨“ç·´...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰æª¢æŸ¥é»
        latest_checkpoint = os.path.join(self.models_dir, "latest_checkpoint.pt")
        if os.path.exists(latest_checkpoint):
            self.load_training_checkpoint(latest_checkpoint)
        
        # åˆå§‹åŒ–episodeçå‹µ
        self.episode_total_reward = 0
        
        print(f"ç•¶å‰Episode: {self.episode_count}")
        
        while self.robot.step(self.timestep) != -1:
            # æ§åˆ¶å¯¦é©—å¹³å°
            self.control_platform()
            
            # ç²å–æ„Ÿæ¸¬å™¨æ•¸æ“š
            imu_data = self.get_imu_data()
            raw_imu_data = self.get_raw_imu_data()
            
            # æ›´æ–°ç‹€æ…‹ç·©å­˜
            self.state_buffer.append(imu_data)
            
            # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
            if self.check_termination_conditions(raw_imu_data):
                # âœ… åœ¨resetä¹‹å‰å…ˆå®Œæˆç•¶å‰æ­¥çš„æ•¸æ“šæ”¶é›†
                
                # ç²å–Transformerå‹•ä½œï¼ˆå³ä½¿çµ‚æ­¢ä¹Ÿè¦å®Œæ•´æ”¶é›†é€™ä¸€æ­¥çš„æ•¸æ“šï¼‰
                transformer_corrections = self.get_transformer_action()
                self.action_buffer.append(transformer_corrections)
                
                # è¨ˆç®—æœ€çµ‚çå‹µ
                current_reward = self.calculate_reward(raw_imu_data)
                self.reward_buffer.append(np.array([current_reward]))
                self.episode_rewards.append(current_reward)
                self.episode_total_reward += current_reward
                
                # æ¨™è¨˜ç‚ºçµ‚æ­¢
                self.episode_dones.append(1.0)
                
                # æ‡‰ç”¨æ§åˆ¶æŒ‡ä»¤ï¼ˆæœ€å¾Œä¸€æ¬¡ï¼‰
                self.apply_fixed_angles_with_corrections(transformer_corrections)
                
                # ç„¶å¾Œé‡ç½®episode
                self.reset_episode()
                continue
            
            # éçµ‚æ­¢æƒ…æ³çš„æ­£å¸¸æµç¨‹
            self.episode_dones.append(0.0)
            
            # ç²å–Transformerå‹•ä½œ
            transformer_corrections = self.get_transformer_action()
            
            # æ›´æ–°å‹•ä½œç·©å­˜
            self.action_buffer.append(transformer_corrections)
            
            # è¨ˆç®—çå‹µ
            current_reward = self.calculate_reward(raw_imu_data)
            self.reward_buffer.append(np.array([current_reward]))
            self.episode_rewards.append(current_reward)
            self.episode_total_reward += current_reward
            
            # æ‡‰ç”¨æ§åˆ¶æŒ‡ä»¤
            self.apply_fixed_angles_with_corrections(transformer_corrections)
            
            # æ›´æ–°è¨ˆæ•¸å™¨
            self.current_step += 1
            self.total_steps += 1
            
            # å®šæœŸè¼¸å‡ºè¨“ç·´ä¿¡æ¯
            if self.current_step % 200 == 0:
                print(f"Episode {self.episode_count}, Step {self.current_step}, "
                      f"ç•¶å‰çå‹µ: {current_reward:.4f}, "
                      f"ç´¯ç©çå‹µ: {self.episode_total_reward:.4f}")
                print(f"IMU: Roll={raw_imu_data[0]:.3f}, Pitch={raw_imu_data[1]:.3f}")
                print(f"ä¿®æ­£é‡ç¯„åœ: {np.min(transformer_corrections):.3f} ~ {np.max(transformer_corrections):.3f}")
            
            # å®šæœŸä¿å­˜æª¢æŸ¥é»
            if self.total_steps % 2000 == 0:
                self.save_training_checkpoint()
                print(f"å·²ä¿å­˜æª¢æŸ¥é» (ç¸½æ­¥æ•¸: {self.total_steps})")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    try:
        controller = HexapodExperimentalController()
        controller.run()
    except KeyboardInterrupt:
        print("\nè¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nè¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ç¨‹åºçµæŸ")